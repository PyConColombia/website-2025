[
  {
    "id": 1,
    "title": {
      "en": "Unpacking the Threat: Malicious Packages in Pypi",
      "es": "Desempaquetando la Amenaza: Paquetes Maliciosos en PyPI"
    },
    "speakers": ["david-cortez"],
    "spoken_language": "english",
    "submission": "talk",
    "description": {
      "en": "This presentation covers various aspects of malicious packages in PyPI, including the techniques used by attackers to inject harmful code into legitimate packages, the potential consequences of using these packages in real-world applications, and the challenges that the community faces in identifying and mitigating such threats. Finally, it explores a real-world case study in which a malicious package infiltrated PyPI.",
      "es": "Esta presentación cubre varios aspectos de los paquetes maliciosos en PyPI, incluidas las técnicas utilizadas por los atacantes para inyectar código dañino en paquetes legítimos, las posibles consecuencias de usar estos paquetes en aplicaciones reales y los desafíos que enfrenta la comunidad para identificar y mitigar tales amenazas. Finalmente, explora un caso de estudio real en el que un paquete malicioso se infiltró en PyPI."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Security"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 2,
    "title": {
      "en": "Arquitecturas Limpias con FastAPI",
      "es": "Arquitecturas Limpias con FastAPI"
    },
    "speakers": ["sergio-infante"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "This talk analyzes how to apply Clean Architecture principles in FastAPI projects to improve maintainability, scalability, and code quality. It covers separation of concerns, business logic organization, best practices for integrating with external services and databases, and strategies for testing and refactoring, leveraging FastAPI's native capabilities for data validation and dependency injection.",
      "es": "En esta charla se analizará cómo aplicar los principios de la Arquitectura Limpia en proyectos construidos con FastAPI, con el objetivo de mejorar la mantenibilidad, escalabilidad y calidad del código. Se abordará la separación de responsabilidades en capas, la organización de la lógica de negocio y las buenas prácticas de integración con servicios externos y bases de datos. Además, se discutirán estrategias de testing y refactorización, aprovechando las capacidades nativas de FastAPI para la validación de datos y la inyección de dependencias."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Web", "Software Architecture"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 3,
    "title": {
      "en": "Crea, Innova y Presenta: Data Storytelling + A.I.",
      "es": "Crea, Innova y Presenta: Data Storytelling + A.I."
    },
    "speakers": ["francisco-alfaro-medina"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "Data alone does not generate impact; it needs a story to be understood. Data Storytelling transforms complex data into accessible narratives, combining analysis, visualization, and narrative to communicate insights effectively. This talk explores how AI revolutionizes Data Storytelling, automating content generation, personalizing visualizations, and optimizing communication for different audiences. Practical applications with Streamlit, Quarto, ChatGPT, and Napkin AI. Learn to create effective visualizations and interactive narratives for all audiences.",
      "es": "Los datos por sí solos no generan impacto; necesitan una historia para ser comprendidos. El Data Storytelling transforma datos complejos en narrativas visuales y accesibles, combinando análisis, visualización y narrativa para comunicar insights de manera efectiva. En esta charla, exploraremos cómo la inteligencia artificial (IA) revoluciona el Data Storytelling, automatizando la generación de contenido, personalizando visualizaciones y optimizando la comunicación de datos a distintas audiencias. Aplicaremos los cuatro pilares de visualización de Noah Iliinsky—propósito, contenido, estructura y formato—para garantizar que cada visualización no solo informe, sino que también conecte con la audiencia de forma efectiva. ¿Qué aprenderás? Cómo la IA mejora la narrativa basada en datos y facilita su interpretación. Aplicaciones prácticas con Streamlit, Quarto, ChatGPT y Napkin AI. Creación de visualizaciones efectivas alineadas con su propósito. Desarrollo de narrativas interactivas que capten la atención y generen impacto. Esta charla está diseñada para todo público, desde científicos de datos y analistas hasta comunicadores, docentes y entusiastas, que deseen transformar datos en historias claras, atractivas y accesibles."
    },
    "summary": { "en": "", "es": "" },
    "tags": [
      "Artificial intelligence",
      "Data Science",
      "Community",
      "Machine Learning",
      "Scientific Computing",
      "Data Storytelling"
    ],
    "level": "all",
    "video_url": ""
  },
  {
    "id": 4,
    "title": {
      "en": "Machine Learning Automágico: Explorando Herramientas de AutoML para Pythonistas Curiosos",
      "es": "Machine Learning Automágico: Explorando Herramientas de AutoML para Pythonistas Curiosos"
    },
    "speakers": ["jorge-emiro-lopez-amaya"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "Imagine building ML models without worrying about choosing the best algorithm or manually tuning hyperparameters! This talk explores the world of AutoML, a set of tools designed to automate model development, letting you focus on making your data speak. We'll compare libraries like TPOT, FLAML, PyCaret, Auto-Sklearn, and more, discussing their strengths, weaknesses, and practical considerations.",
      "es": "¿Te imaginas construir modelos de Machine Learning sin preocuparte por elegir el mejor algoritmo o ajustar hiperparámetros manualmente? ¡Es posible y más fácil de lo que crees! En esta charla, exploraremos el fascinante mundo de AutoML, una colección de herramientas diseñadas para automatizar el desarrollo de modelos de aprendizaje automático, permitiendo que te concentres en lo que realmente importa: ¡hacer que tus datos hablen! A través de una comparación práctica, veremos cómo diferentes librerías como TPOT, FLAML, PyCaret, Auto-Sklearn y muchas más pueden facilitar nuestra vida como desarrolladores. También hablaremos de sus fortalezas, debilidades y qué considerar antes de integrarlas en proyectos reales. Si eres un apasionado del Machine Learning, un entusiasta de Python o simplemente quieres descubrir nuevas formas de potenciar tu flujo de trabajo, esta charla es para ti. ¡Prepárate para desafiar la forma en la que construimos modelos y descubrir cómo la automatización puede llevar tu código al siguiente nivel! "
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Artificial intelligence", "Data Science", "Machine Learning"],
    "level": "beginner",
    "video_url": ""
  },
  {
    "id": 5,
    "title": {
      "en": "why not combine Python with C, C++, Java, Rust? and how?",
      "es": "¿Por qué no combinar Python con C, C++, Java, Rust? ¿Y cómo?"
    },
    "speakers": ["daniel-arango-sohm"],
    "spoken_language": "english",
    "submission": "talk",
    "description": {
      "en": "Python is more than just a scripting language—it’s the ultimate glue that binds high-performance languages like C, C++, Java, Rust, and JavaScript together. In this talk, we’ll explore real-world, practical integrations where Python plays a central role in orchestrating complex applications across multiple languages. We'll dive into projects such as: Network Sniffers leveraging C for speed and Python for analysis. 3D File Systems using Python for API development and JavaScript for visualization. Bioinformatics Libraries in Rust and Python, combining safety with expressiveness. Algorithm Libraries where Python and Rust enhance performance. Process Monitors using C++ for efficiency and Python for accessibility. Custom Assembler Simulations with Java for low-level logic and Python for scripting. Animations & Interactive Code with JavaScript for UI enhancements. This talk will provide insights into why and how to bridge Python with other languages, highlighting practical tools like PyO3, pybind11, Jpype, ctypes, and more.",
      "es": "Python es más que un lenguaje de scripting: es el pegamento definitivo que une lenguajes de alto rendimiento como C, C++, Java, Rust y JavaScript. En esta charla, exploraremos integraciones prácticas y reales donde Python juega un papel central en la orquestación de aplicaciones complejas entre múltiples lenguajes. Veremos proyectos como: Network Sniffers usando C para velocidad y Python para análisis. Sistemas de archivos 3D con Python para APIs y JavaScript para visualización. Librerías de bioinformática en Rust y Python, combinando seguridad y expresividad. Librerías de algoritmos donde Python y Rust mejoran el rendimiento. Monitores de procesos usando C++ para eficiencia y Python para accesibilidad. Simuladores de ensamblador personalizados con Java para lógica de bajo nivel y Python para scripting. Animaciones y código interactivo con JavaScript para UI. Esta charla te dará herramientas prácticas para integrar Python con otros lenguajes, destacando PyO3, pybind11, Jpype, ctypes y más."
    },
    "summary": { "en": "", "es": "" },
    "tags": [
      "Core Python",
      "Scientific Computing",
      "Web",
      "Operating systems",
      "rust",
      "networks",
      "java",
      "c++",
      "low leve",
      "c"
    ],
    "level": "advanced",
    "video_url": ""
  },
  {
    "id": 6,
    "title": {
      "en": "Python para hacking de machine learning",
      "es": "Python para hacking de machine learning"
    },
    "speakers": ["christian-camilo"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "Artificial intelligence (AI) has gained great relevance in various businesses and daily operations, bringing new opportunities and challenges in cybersecurity. This talk introduces the current context of cybersecurity in AI, tools, and practices to reduce risks in information systems, with a focus on offensive security using Python for exploitation, evasion, poisoning, extraction, and inference techniques.",
      "es": "La inteligencia artificial (IA) ha ganado una gran relevancia en distintos tipos de negocio (desde salud, banca, entretenimiento, etc.) y en nuestros operaciones del día a día. Lo anterior ha vislumbrado nuevas oportunidades en ciberseguridad y del mismo modo un conjunto de retos que aún hoy por hoy son desconocidos. La seguridad ha cambiado debido a las incorporaciones de la IA en nuestras operaciones, implicando una transformación tanto en los roles y los procesos tradicionales; lo anterior dado a los nuevos riesgos que podrían ser incidentes cibernéticos, eventos que pueden efectuar daños e implicaciones éticas aun en exploración tanto por la academia y la industria. En los últimos años se ha evidenciado una necesidad de personas con conocimientos en ciberseguridad, ahora será importante no solo lo anterior sino que también conozcan sobre IA, lo anterior da a conocer la relevancia de empezar a construir elementos que le permitan a las personas conocer el contexto actual de ciberseguridad en inteligencia artificial, las herramientas actuales y las practicas necesarias para reducir los riesgos que se puedan presentar en los sistemas informáticos. Desde las experiencias adquiridas en el campo de la ética de inteligencia artificial y de once años de investigación en ciberseguridad e inteligencia artificial, presentamos un contenido que introduce a cualquier persona interesada en el campo ofensivo de ciberseguridad para inteligencia artificial; la charla incluye un contexto sobre ciberseguridad en inteligencia artificial, haciendo un énfasis en al aspecto ofensivo bajo el marco de referencia de Atlas MITRE ATT&CK, la presentación aborda el uso de python para la explotación de los riesgos presentados por OWASP top 10 tanto en modelos tradicionales y generativos. Dentro de la charla se presentarán algunos ejemplos de técnicas de evasión, envenenamiento, extracción e inferencia abordados a través de los frameworks de Python -Giskard y ART-. Posteriormente, también se presentarán los resultados que hemos conseguido en distintos proyectos de investigación a nivel ofensivo, por ejemplo el desarrollo de una IA con comportamiento de ransomware y otra para perfilamiento tanto de personas y organizaciones. Como parte de los once años de investigación en el campo, también estaré otorgando algunos libros impresos a los asistentes con el fin que conozcan un poco más sobre los desarrollos que se han logrado en Latinoamérica tanto en la perspectiva defensiva como en la ofensiva de ciberseguridad e IA; de los anteriores cabe citar: nuestro primer libro “Ciberseguridad: un enfoque desde la ciencia de datos” publicado en 2018, el “Ciberseguridad: los datos tienen la respuesta” en 2022, y finalmente “Ciberseguridad: los datos, las semillas del caos” en 2023."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Artificial intelligence", "Data Science", "Machine Learning", "Cybersecurity"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 7,
    "title": {
      "en": "Beyond Pretty Charts: Storytelling with Data Viz in Python",
      "es": "Más allá de los gráficos bonitos: Storytelling con Data Viz en Python"
    },
    "speakers": ["catalina-naranjo"],
    "spoken_language": "english",
    "submission": "talk",
    "description": {
      "en": "Have you ever created a technically correct chart that no one seemed to understand—or care about? Or built a dashboard that ended up ignored? It’s not just about visualizing data; it’s about communicating meaning. In this talk, we’ll explore how to move from displaying data to telling compelling stories with it, using Python libraries like Plotly, Altair, and Streamlit.",
      "es": "¿Alguna vez creaste un gráfico técnicamente correcto que nadie pareció entender o al que nadie le prestó atención? ¿O construiste un dashboard que terminó siendo ignorado? No se trata solo de visualizar datos, sino de comunicar significado. En esta charla, exploraremos cómo pasar de mostrar datos a contar historias convincentes con ellos, usando librerías de Python como Plotly, Altair y Streamlit."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Data Science", "Data Viz", "Insights Generation", "Storytelling"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 8,
    "title": {
      "en": "Sinfonía de Mentes Artificiales: Orquestando Agentes Cognitivos con LangChain y Python para Construir Ecosistemas Inteligentes",
      "es": "Sinfonía de Mentes Artificiales: Orquestando Agentes Cognitivos con LangChain y Python para Construir Ecosistemas Inteligentes"
    },
    "speakers": ["catalina-cruz"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "Imagine a team of cognitive agents collaborating to answer questions, summarize documents, and automate complex processes. This talk covers how to orchestrate multiple language models and specialized tools using Python and LangChain, with real-world use cases and live code demos.",
      "es": "¿Te imaginas un equipo de agentes cognitivos colaborando para responder preguntas, resumir documentos y automatizar procesos complejos? En esta charla aprenderemos cómo unir varios modelos de lenguaje y herramientas especializadas para resolver tareas en conjunto, aprovechando la flexibilidad de Python y la potencia de LangChain. Hablaremos sobre: Fundamentos de Agentes Cognitivos, Arquitectura con LangChain, Manejo de Flujos y Errores, Casos de Uso Reales y Demostraciones Prácticas. Al finalizar, tendrás la hoja de ruta para construir ecosistemas de IA compuestos por múltiples agentes que colaboran como si fueran un solo cerebro."
    },
    "summary": { "en": "", "es": "" },
    "tags": [
      "Artificial intelligence",
      "Data Science",
      "Community",
      "Machine Learning",
      "Scientific Computing"
    ],
    "level": "all",
    "video_url": ""
  },
  {
    "id": 9,
    "title": {
      "en": "Python y Machine Learning en Agronomía",
      "es": "Python y Machine Learning en Agronomía"
    },
    "speakers": ["david-roa"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "This talk presents an advanced approach to using Python and Machine Learning to estimate critical nutrients in plant tissues, leveraging correlations between elements measured by analytical techniques. Real cases in crops like corn, soy, and alfalfa in the US Midwest, and sugarcane in Colombia, are discussed, along with technical and economic challenges.",
      "es": "La agricultura siempre ha estado rezagada en la tecnología. Las aplicaciones más comunes siguen siendo pocas y principalmente en el área de computer vision, IOT. Quisimos trabajar este proyecto desde la mirada de la química analítica y de cómo podemos aprovechar los datos de mediciones de espectrometría de masas para proponer soluciones reales en agricultura. Medir con precisión nutrientes en tejidos vegetales es crucial, pero extremadamente costoso, complejo y lento, sobre todo cuando las concentraciones están en partes por millón (ppm). Esta charla presenta un enfoque avanzado sobre cómo usamos Python y Machine Learning para estimar con precisión estos nutrientes críticos, aprovechando correlaciones entre un total de 11 elementos de la tabla periódica medidos por técnicas analíticas como ICP-MS y XRF portátil. Usando casos reales en cultivos como Maíz, Soya y Alfalfa del Midwest estadounidense, y ahora en Caña de azúcar del Valle del Cauca en Colombia, explicaré detalladamente cómo preprocesamos muestras vegetales, desde el secado y molienda hasta su análisis instrumental, resaltando los retos técnicos y económicos que esto implica. Luego mostraré cómo, a través de técnicas avanzadas de imputación (usando modelos de ML como: KNN, SVD, Random Forest, entre muchos otros) y análisis multivariantes, generamos modelos predictivos robustos (Random Forest, SVR, Gradient Boosting, entre otros) para todos los nutrientes evaluados. Finalmente, compartiré cómo estas predicciones se traducen en recomendaciones prácticas para una fertilización precisa y sustentable, identificando excesos, deficiencias y condiciones óptimas en tiempo real, potenciando así una agricultura más eficiente y responsable ambientalmente."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Data Science", "Machine Learning", "Scientific Computing"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 10,
    "title": {
      "en": "Anotaciones de tipos en Python: Escribe código más robusto y mantenible",
      "es": "Anotaciones de tipos en Python: Escribe código más robusto y mantenible"
    },
    "speakers": ["luis-martinez"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "This talk shows how to leverage Python's type annotation system to write safer, more maintainable, and editor-friendly code. From basics to modern tools like mypy, typeguard, and pydantic, with practical examples and real-world improvements.",
      "es": "¿Alguna vez te encontraste con un error absurdo en producción que podría haberse evitado con una simple validación de tipos? ¿Tu editor no te sugiere nada útil y terminas revisando la documentación más de lo que quisieras? En esta charla te mostraré cómo aprovechar el sistema de anotaciones de tipo de Python para escribir código más seguro, mantenible y autocompletado-friendly. Exploraremos desde lo más básico (¿qué tipos existen en Python?) hasta herramientas modernas como mypy, typeguard y pydantic. Verás ejemplos prácticos, comparaciones reales y mejoras inmediatas al trabajar en tu editor favorito (sí, hay demos visuales con VSCode). Esta charla está pensada tanto para personas que recién están empezando con Python como para desarrolladores con experiencia que quieren mejorar la calidad de su código sin perder la flexibilidad del lenguaje. Si alguna vez pensaste que el tipado estático no era para ti, esta charla podría hacerte cambiar de opinión."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Core Python"],
    "level": "beginner",
    "video_url": ""
  },
  {
    "id": 11,
    "title": {
      "en": "Engineering Effective Enterprise AI: Leveraging CAG, RAG, and MCP hybrid architectures for Automation & Human Augmentation Systems",
      "es": "Engineering Effective Enterprise AI: Leveraging CAG, RAG, and MCP hybrid architectures for Automation & Human Augmentation Systems"
    },
    "speakers": ["rei-romero"],
    "spoken_language": "english",
    "submission": "talk",
    "description": {
      "en": "This talk explores advanced architectural approaches for enterprise AI implementation with real-world insights, that go beyond basic chatbots and single-model solutions. You'll learn how Context-Aware Generation (CAG), Retrieval-Augmented Generation (RAG), and Multi-Context Protocols (MCP) can be combined into hybrid architectures for production enterprise systems, leveraging automation & human augmentation to drive sustainable impact. We'll examine: Technical decision frameworks for selecting optimal AI approaches, implementation patterns for human-centered automation systems, orchestrating multi-agent systems and data-sources in production, lessons learned from real-world case studies, sharing successes and failures on enterprise deployments, and common pitfalls and how to avoid them. You'll walk away with practical knowledge about designing effective AI systems that balance automation with human augmentation, applicable to both technical implementations and strategic planning.",
      "es": "Esta charla explora enfoques arquitectónicos avanzados para la implementación de IA empresarial con ideas del mundo real, que van más allá de los chatbots básicos y las soluciones de un solo modelo. Aprenderás cómo la Generación Consciente del Contexto (CAG), la Generación Aumentada por Recuperación (RAG) y los Protocolos de Contexto Múltiple (MCP) pueden combinarse en arquitecturas híbridas para sistemas empresariales de producción, aprovechando la automatización y la mejora humana para impulsar un impacto sostenible. Examinaremos: marcos de decisión técnica para seleccionar enfoques óptimos de IA, patrones de implementación para sistemas de automatización centrados en el ser humano, orquestación de sistemas multiagente y fuentes de datos en producción, lecciones aprendidas de estudios de caso del mundo real, compartiendo éxitos y fracasos en implementaciones empresariales, y errores comunes y cómo evitarlos. Te irás con conocimientos prácticos sobre cómo diseñar sistemas de IA efectivos que equilibren la automatización con la mejora humana, aplicables tanto a implementaciones técnicas como a la planificación estratégica."
    },
    "summary": { "en": "", "es": "" },
    "tags": [
      "Artificial intelligence",
      "Community",
      "Systems Architecture",
      "Enterprise",
      "Use Cases",
      "Startups"
    ],
    "level": "all",
    "video_url": ""
  },
  {
    "id": 12,
    "title": {
      "en": "¿Y si entrenamos modelos sin ver los datos? Federated Learning",
      "es": "¿Y si entrenamos modelos sin ver los datos? Federated Learning"
    },
    "speakers": ["nicolas-danies"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "What if we could train models without seeing the data? This talk explores Federated Learning, a technique that allows training machine learning models without centralizing information. Through practical examples and accessible explanations, you'll learn how to build solutions that respect user privacy without sacrificing performance. Includes a step-by-step implementation using Flower and real-world use cases in health, finance, and mobile devices.",
      "es": "¿Y si pudiéramos entrenar modelos sin ver los datos? En esta charla exploraremos Federated Learning, una técnica que permite entrenar modelos de machine learning sin necesidad de centralizar la información. A través de ejemplos prácticos y explicaciones accesibles, mostraré cómo construir soluciones que respetan la privacidad de los usuarios sin sacrificar rendimiento. Presentaré una implementación paso a paso usando Flower, uno de los frameworks más utilizados hoy en día para sistemas federados, y discutiremos cómo adaptar esta tecnología a distintos escenarios. Además, compartiré casos de uso reales donde el aprendizaje federado ya está siendo aplicado en sectores como salud, finanzas y dispositivos móviles, y cómo me imagino que se puede empezar a utilizar en Colombia. También hablaremos sobre cómo mejorar aún más la privacidad en estos sistemas mediante estrategias complementarias, sin perder de vista la viabilidad técnica y operativa. Esta charla está pensada para cualquier persona interesada en machine learning, privacidad o arquitectura de sistemas distribuidos. Si alguna vez te has preguntado cómo entrenar modelos responsables, escalables y preparados para un futuro más ético, esta charla es para ti."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Artificial intelligence", "Data Science", "Machine Learning", "MLOps"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 13,
    "title": {
      "en": "ARGUS: análisis geoespacial y grafos para enfrentar la deforestación y la ganadería ilegal en Colombia",
      "es": "ARGUS: análisis geoespacial y grafos para enfrentar la deforestación y la ganadería ilegal en Colombia"
    },
    "speakers": ["esteban-gonzalez"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "In this talk I will present ARGUS, a geospatial monitoring tool developed in Python that combines remote sensors, data analysis, and graph structures to detect and analyze patterns of illegal cattle ranching linked to deforestation in Colombia. We will explore how this solution, designed from the public sector, uses clustering algorithms, interactive visualization, and network detection to reveal hidden dynamics in the territory, optimizing environmental decision-making. If you are interested in open data, applied science with real impact, and the potential of Python to solve complex social problems, this talk is for you.",
      "es": "En esta charla presentaré ARGUS, una herramienta de monitoreo geoespacial desarrollada en Python que combina sensores remotos, análisis de datos y estructuras de grafos para detectar y analizar patrones de ganadería ilegal vinculados a la deforestación en Colombia. Exploraremos cómo esta solución, pensada desde el sector público, utiliza algoritmos de agrupamiento, visualización interactiva y detección de redes para revelar dinámicas ocultas en el territorio, optimizando la toma de decisiones ambientales. Si te interesan los datos abiertos, la ciencia aplicada con impacto real y el potencial de Python para resolver problemáticas sociales complejas, esta charla es para ti."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Artificial intelligence", "Data Science", "Computer Vision", "Machine Learning"],
    "level": "advanced",
    "video_url": ""
  },
  {
    "id": 14,
    "title": {
      "en": "How to pass your job interview in English at a multinational company?",
      "es": "¿Cómo pasar tu entrevista laboral en inglés en una empresa multinacional?"
    },
    "speakers": ["nataliya-ershova"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "Have you ever felt that your English level held you back in an interview, even though you had all the technical skills? Have you frozen when asked 'Tell me about yourself'? You're not alone. Worried about your accent, technical vocabulary, or just not understanding the questions? This talk is designed especially for you: Latin American professionals who dream of joining global teams and want to overcome their fears of English in technical interviews. I'll share lessons from my experience as an AI team leader with Latin American talent, former English teacher, and career coach. The session will focus on the most common challenges Spanish-speaking candidates face: fear of making mistakes, lack of technical vocabulary, nerves, and doubts about how to structure effective answers in English. The agenda includes: strategies for preparing for technical and behavioral interviews in English; practical tools to communicate ideas clearly, even with intermediate English; reflections on how to address fear through practice and confidence, not perfection; real cases of candidate preparation in Colombia and Latin America. The talk offers a practical and realistic guide, combining linguistic preparation with an emotional perspective, aimed at those who want to improve their interview performance without having to master native-level English.",
      "es": "Alguna vez sentiste que tu nivel de inglés te frenó en una entrevista, aunque tenías todas las habilidades técnicas necesarias? Te has paralizado cuando te preguntan “Tell me about yourself”? No estás solo. Te preocupa tu acento, tu vocabulario técnico o simplemente no entender las preguntas? Esta charla está diseñada especialmente para ti: profesionales latinoamericanos que sueñan con dar el salto a equipos globales y quieren superar sus miedos al inglés en entrevistas técnicas. Compartiré aprendizajes desde mi experiencia como líder de un equipo de inteligencia artificial con talento latinoamericano, exprofesora de inglés y coach de carrera en un bootcamp. La sesión se centrará en los desafíos más comunes que enfrentan los candidatos hispanohablantes: miedo a equivocarse, falta de vocabulario técnico, nerviosismo y dudas sobre cómo estructurar respuestas efectivas en inglés. La agenda incluirá: Estrategias para prepararse para entrevistas técnicas y de comportamiento en inglés; Herramientas prácticas para comunicar ideas con claridad, incluso con un nivel de inglés intermedio; Reflexiones sobre cómo abordar el miedo desde la práctica y la confianza, no desde la perfección; Casos reales de preparación de candidatos en Colombia y América Latina. La charla busca ofrecer una guía práctica y realista, que combina preparación lingüística con perspectiva emocional, orientada a quienes desean mejorar su desempeño en entrevistas sin tener que dominar el inglés a nivel nativo."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Artificial intelligence", "Community", "Soft skills", "English"],
    "level": "all",
    "video_url": ""
  },
  {
    "id": 15,
    "title": {
      "en": "Python for Observability: Building Your Own Monitoring Platform with Open Source",
      "es": "Python para Observabilidad: Construyendo tu Propia Plataforma de Monitoreo con Open Source"
    },
    "speakers": ["ana-valencia"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "In this talk, we'll learn how to use Python to build your own monitoring platform without relying on commercial solutions. Using open source tools like psutil, prometheus_client, FastAPI, and Grafana, we'll explore how to collect, expose, and visualize system metrics in real time. Step by step, we'll create a Python 'exporter' that reports CPU, memory, disk, network, and other indicators, and see how to easily integrate it with Prometheus and Grafana for useful dashboards and configurable alerts. This is a practical talk, focused on showing how a lightweight and flexible solution can give you real observability of your services—ideal for backend developers, DevOps, or anyone who wants to better understand their infrastructure.",
      "es": "En esta charla aprenderemos cómo usar Python para construir tu propia plataforma de monitoreo sin depender de soluciones comerciales. A través de herramientas de código abierto como psutil, prometheus_client, FastAPI y Grafana, exploraremos cómo recolectar, exponer y visualizar métricas del sistema en tiempo real. Veremos paso a paso cómo crear un 'exportador' en Python que reporte el uso de CPU, memoria, disco, red y otros indicadores, y cómo integrarlo fácilmente con Prometheus y Grafana para tener dashboards útiles y alertas configurables. Esta es una charla práctica, enfocada en mostrar cómo una solución ligera y flexible puede darte observabilidad real de tus servicios, ideal para quienes trabajan en desarrollo backend, DevOps o simplemente quieren entender mejor el estado de su infraestructura."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Devops", "Observability and monitoring", "Python"],
    "level": "all",
    "video_url": ""
  },
  {
    "id": 16,
    "title": {
      "en": "Understanding geospatial data with duckdb",
      "es": "Understanding geospatial data with duckdb"
    },
    "speakers": ["jorge-martinez"],
    "spoken_language": "english",
    "submission": "talk",
    "description": {
      "en": "Geospatial data can be defined as information regarding objects or events that include location as additional attributes. This means that you can map features that represent real-world phenomena into latitude and longitude coordinates from earth’s surface. With this, you can perform analysis, generate insights and create a clear picture of a given situation. For example, in an emergency context, humanitarian organizations apply geospatial analysis to find health facilities nearby an earthquake epicenter, improving operations while reducing response times. We can leverage the understanding of different circumstances by applying location information in a given context. However, it becomes necessary to have well-defined concepts and terminology. Working with geospatial data requires a minimum basis that can facilitate the daily work of many developers and analysts. In this talk, I will provide an introduction of performing taks and working with geospatial data using duckdb. We will take advantage of this modern tool, that provides it users the ease of getting started, working with it and also integrate with different programming languages, such as Python. At the end of this talk, attendees will have a clear view about what is geospatial data, how to collect it and run geospatial operations. I will select common and open data sources used in emergency context. However, the knowledge can be re-used in any real world scenario. The schedule of this talk is the following: Installation of duckdb (10 seconds). Basics of GIS (10 minutes). Loading geospatial data in duckdb (5 minutes) using custom functions. Operations with vector data (intersection of polygons, aggregation of points, etc.) (10 minutes). Data export using geospatial standards (3 minutes). Common issues found (2 minutes).",
      "es": "Geospatial data can be defined as information regarding objects or events that include location as additional attributes. This means that you can map features that represent real-world phenomena into latitude and longitude coordinates from earth’s surface. With this, you can perform analysis, generate insights and create a clear picture of a given situation. For example, in an emergency context, humanitarian organizations apply geospatial analysis to find health facilities nearby an earthquake epicenter, improving operations while reducing response times. We can leverage the understanding of different circumstances by applying location information in a given context. However, it becomes necessary to have well-defined concepts and terminology. Working with geospatial data requires a minimum basis that can facilitate the daily work of many developers and analysts. In this talk, I will provide an introduction of performing taks and working with geospatial data using duckdb. We will take advantage of this modern tool, that provides it users the ease of getting started, working with it and also integrate with different programming languages, such as Python. At the end of this talk, attendees will have a clear view about what is geospatial data, how to collect it and run geospatial operations. I will select common and open data sources used in emergency context. However, the knowledge can be re-used in any real world scenario. The schedule of this talk is the following: Installation of duckdb (10 seconds). Basics of GIS (10 minutes). Loading geospatial data in duckdb (5 minutes) using custom functions. Operations with vector data (intersection of polygons, aggregation of points, etc.) (10 minutes). Data export using geospatial standards (3 minutes). Common issues found (2 minutes)."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Data Science", "Scientific Computing"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 17,
    "title": {
      "en": "How Not to Write Async Python (and What to Do Instead)",
      "es": "How Not to Write Async Python (and What to Do Instead)"
    },
    "speakers": ["juan-vanegas"],
    "spoken_language": "english",
    "submission": "talk",
    "description": {
      "en": "Async Python has been around for a while, and while many developers use async/await, few truly write asynchronous code. This talk demystifies async programming by exploring the differences between sync, async, and parallel execution, explaining how the event loop works, and identifying common pitfalls like excessive awaits blocking the event loop. Attendees will learn to fix bad async patterns using tools like asyncio.gather(), create_task(), TaskGroup, and semaphores to manage concurrency effectively. Through real-world examples, we’ll refactor inefficient async code into structured, performant solutions. By the end, developers will not only understand when and how to use async but also write actual async Python—beyond just syntax.",
      "es": "Async Python has been around for a while, and while many developers use async/await, few truly write asynchronous code. This talk demystifies async programming by exploring the differences between sync, async, and parallel execution, explaining how the event loop works, and identifying common pitfalls like excessive awaits blocking the event loop. Attendees will learn to fix bad async patterns using tools like asyncio.gather(), create_task(), TaskGroup, and semaphores to manage concurrency effectively. Through real-world examples, we’ll refactor inefficient async code into structured, performant solutions. By the end, developers will not only understand when and how to use async but also write actual async Python—beyond just syntax."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["aCore Python"],
    "level": "all",
    "video_url": ""
  },
  {
    "id": 18,
    "title": {
      "en": "From Spreadsheets to Algorithms: How Finance Led Me to Artificial Intelligence?",
      "es": "De las hojas de cálculo a los algoritmos: cómo las finanzas me llevaron a la inteligencia artificial?"
    },
    "speakers": ["carlos-ramirez"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "Can you imagine transforming decades of financial experience, dominated by omnipresent Excel, into an engine of innovation powered by Python and Artificial Intelligence? That's the story Carlos Mario Ramírez Gil, with over 25 years in finance and author of 'Python para finanzas, Curso Práctico', comes to share. Many professionals live the reality of spreadsheets: powerful, yes, but often limiting in the face of modern data complexity and volume. Carlos Mario was there. But six years ago, he decided to break that mold. In this talk, he'll take you on a fascinating and practical journey: The Excel Wall: We'll revisit the concrete limitations found in traditional financial analysis that cry out for a more robust solution. (You'll surely relate!) The Discovery of Python: You'll witness how Python burst in to automate the tedious, analyze the complex, and unlock insights previously hidden in the numbers. We'll see practical examples (without drowning in code!) of how financial tasks were revolutionized. Mastering Finance with Python: We'll explore how tools like Pandas, NumPy, and others became indispensable allies for modeling, simulation, and advanced quantitative analysis, far surpassing previous capabilities. The Quantum Leap to AI: Discover how curiosity and necessity led Carlos Mario to apply Machine Learning algorithms (Regression, Time Series, Optimization!) to tackle even greater financial challenges: from predicting cryptocurrency trends to optimizing portfolios and analyzing complex patterns in urban data. Your Own Transformation: Beyond the story, you'll get lessons learned, practical tips, and inspiration to start or accelerate your own path integrating Python and AI into your professional domain, whether finance or another field. Why can't you miss this talk? Real Experience, No Smoke: It's not abstract theory, it's the experience of a professional who made the transition. Practical Applications: You'll see how Python and AI solve real problems in the financial sector. Pure Inspiration: It shows that technological reinvention is possible at any age and from any field. Bridge Between Worlds: Ideal whether you come from finance and want to learn Python, or you're a developer looking for impactful applications. Join Carlos Mario to discover how to turn data into smart decisions and how your own career can evolve from spreadsheet cells to the unlimited power of algorithms. A talk that will equip you with perspective, knowledge, and motivation!",
      "es": "¿Imaginas transformar décadas de experiencia financiera, dominadas por el omnipresente Excel, en un motor de innovación impulsado por Python y la Inteligencia Artificial? Esa es la historia que Carlos Mario Ramírez Gil, con más de 25 años en el campo financiero y autor de 'Python para finanzas, Curso Práctico', viene a compartir contigo. Muchos profesionales viven la realidad de las hojas de cálculo: poderosas, sí, pero a menudo limitantes frente a la complejidad y el volumen de los datos modernos. Carlos Mario estaba allí. Pero hace seis años, decidió romper ese molde. En esta charla, te llevará en un viaje fascinante y práctico: El Muro de Excel: Reviviremos las limitaciones concretas encontradas en el análisis financiero tradicional que gritan por una solución más robusta. (¡Seguro te identificarás!) El Descubrimiento de Python: Serás testigo de cómo Python irrumpió para automatizar lo tedioso, analizar lo complejo y desbloquear insights antes ocultos en los números. Veremos ejemplos prácticos (¡sin ahogarnos en código!) de cómo tareas financieras se revolucionaron. Dominando las Finanzas con Python: Exploraremos cómo herramientas como Pandas, NumPy y otras se convirtieron en aliadas indispensables para modelado, simulación y análisis cuantitativo avanzado, superando ampliamente las capacidades previas. El Salto Cuántico a la IA: Descubre cómo la curiosidad y la necesidad llevaron a Carlos Mario a aplicar algoritmos de Machine Learning (¡Regresión, Series de Tiempo, Optimización!) para abordar desafíos financieros aún mayores: desde predecir tendencias en criptomonedas hasta optimizar portafolios y analizar patrones complejos en datos urbanos. Tu Propia Transformación: Más allá de la historia, obtendrás lecciones aprendidas, consejos prácticos y la inspiración para iniciar o acelerar tu propio camino integrando Python y IA en tu dominio profesional, sea finanzas u otro. ¿Por qué no te puedes perder esta charla? Experiencia Real, Cero Humo: No es teoría abstracta, es la vivencia de un profesional que hizo la transición. Aplicaciones Prácticas: Verás cómo Python y la IA resuelven problemas reales del sector financiero. Inspiración Pura: Demuestra que la reinvención tecnológica es posible a cualquier edad y desde cualquier campo. Puente entre Mundos: Ideal tanto si vienes de finanzas y quieres aprender Python, como si eres developer y buscas aplicaciones de impacto. Únete a Carlos Mario para descubrir cómo convertir los datos en decisiones inteligentes y cómo tu propia carrera puede evolucionar de las celdas de una hoja de cálculo a la potencia ilimitada de los algoritmos. ¡Una charla que te equipará con perspectiva, conocimiento y motivación!"
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Artificial intelligence", "Data Science", "Community", "Machine Learning"],
    "level": "all",
    "video_url": ""
  },
  {
    "id": 19,
    "title": {
      "en": "Connecting AI to the Real World: Empowering Agents with the Model Context Protocol (MCP)",
      "es": "Conectando la IA con el Mundo Real: Potenciando Agentes con el Model Context Protocol (MCP)"
    },
    "speakers": ["jose-coronado"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "Imagine connecting your AI agents to databases, APIs, file systems, and web services as easily as plugging in a USB cable. Welcome to the Model Context Protocol (MCP), the revolutionary standard transforming how Large Language Models (LLMs) interact with the real world. In this talk, we'll explore MCP, an open protocol designed to simplify and standardize the integration of AI models with diverse tools and data sources. We'll cover: What MCP is and why it's considered 'the USB-C of AI.' How MCP solves common integration problems—goodbye to fragile, custom APIs! Practical cases and the flourishing open-source ecosystem already leveraging MCP. Whether you're a developer optimizing AI integrations, a data scientist seeking smarter workflows, or simply an enthusiast curious about the future of AI tools, this talk is for you. Discover how MCP makes AI agents more powerful, modular, and truly connected to reality.",
      "es": "Imagina conectar tus agentes de inteligencia artificial con bases de datos, APIs, sistemas de archivos y servicios web con la facilidad de enchufar un cable USB. Bienvenidos al Model Context Protocol (MCP), el revolucionario estándar que está transformando la manera en que los Modelos de Lenguaje Extensos (LLMs) interactúan con el mundo real. En esta charla exploraremos MCP, un protocolo abierto diseñado para simplificar y estandarizar la integración de modelos de IA con diversas herramientas y fuentes de datos. Cubriremos: Qué es MCP y por qué se considera 'el USB-C de la IA.' Cómo MCP resuelve los problemas frecuentes en la integración de IA—¡adiós a las APIs personalizadas y frágiles! Casos prácticos y el floreciente ecosistema de código abierto que ya está aprovechando MCP. Ya seas desarrollador interesado en optimizar integraciones de IA, científico de datos buscando flujos de trabajo más inteligentes, o simplemente un entusiasta deseoso de conocer el futuro de las herramientas de inteligencia artificial, esta charla es para ti. Ven a descubrir cómo MCP hace que los agentes de IA sean más potentes, modulares y verdaderamente conectados a la realidad."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Artificial intelligence"],
    "level": "all",
    "video_url": ""
  },
  {
    "id": 20,
    "title": {
      "en": "Developing Enbloc: Building an AI-Driven Legislative Data Platform",
      "es": "Desarrollando Enbloc: Construyendo una Plataforma de Datos Legislativos Impulsada por IA"
    },
    "speakers": ["diego-ayala", "andres-vasquez"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "In this talk, we will share our journey of developing Enbloc, an AI-powered platform designed to navigate the intricate web of political relationships in Washington, D.C. Enbloc integrates diverse data sources—including congressional records, lobbying reports, and campaign finance data—to provide users with comprehensive insights into Capitol Hill's key players and their connections. Key takeaways: Architectural Insights: Understand the design decisions behind Enbloc's scalable architecture, which is built to handle vast datasets efficiently. AI Integration: Learn how we incorporated machine learning models to analyze and visualize complex political relationships. Real-World Challenges: Discover the hurdles we faced in data integration, user experience design, and system performance, along with the solutions we implemented. By attending, you'll gain practical knowledge on leveraging Python and AI to build applications that process and analyze large-scale, real-time data, offering valuable insights into developing sophisticated data-driven platforms.",
      "es": "En esta charla compartiremos nuestro recorrido desarrollando Enbloc, una plataforma impulsada por IA diseñada para navegar la compleja red de relaciones políticas en Washington, D.C. Enbloc integra diversas fuentes de datos—registros del Congreso, reportes de lobby y datos de financiamiento de campañas—para brindar a los usuarios una visión integral de los actores clave y sus conexiones en Capitol Hill. Puntos clave: Perspectivas arquitectónicas: Entiende las decisiones de diseño detrás de la arquitectura escalable de Enbloc, construida para manejar grandes volúmenes de datos de manera eficiente. Integración de IA: Aprende cómo incorporamos modelos de machine learning para analizar y visualizar relaciones políticas complejas. Retos reales: Descubre los desafíos que enfrentamos en integración de datos, experiencia de usuario y rendimiento del sistema, junto con las soluciones implementadas. Al asistir, obtendrás conocimientos prácticos sobre cómo aprovechar Python y la IA para construir aplicaciones que procesan y analizan datos a gran escala y en tiempo real, ofreciendo valiosos aprendizajes para desarrollar plataformas sofisticadas basadas en datos."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Artificial intelligence", "Computer Vision", "Machine Learning"],
    "level": "all",
    "video_url": ""
  },
  {
    "id": 21,
    "title": {
      "en": "Production-Ready Python RAG Systems: A Clinical Healthcare Perspective",
      "es": "Production-Ready Python RAG Systems: A Clinical Healthcare Perspective"
    },
    "speakers": ["ana-rua", "juan-rua"],
    "spoken_language": "english",
    "submission": "talk",
    "description": {
      "en": "In this talk, we’ll share how we built and deployed a production-grade Retrieval-Augmented Generation (RAG) system using Python to extract clinical data from unstructured medical documents — enabling healthcare teams to map patients, gain real-time insights, and accelerate clinical trial recruitment. We’ll walk through the end-to-end architecture of the system, which processes over 4 million documents per month and combines document parsing, text classification, chunking, embeddings, vector search, and LLM-powered data extraction. You’ll see how Python ties everything together across a modular AWS-based pipeline using services like Lambda, Step Functions, and S3 — all designed for scalability, traceability, and efficiency. This talk will highlight: How we orchestrated a high-throughput document processing workflow in Python. The challenges of deploying LLMs in clinical environments (e.g., hallucinations, evaluation, and prompt design). Lessons learned building a custom RAG system with healthcare-specific use cases. How the solution enables dynamic cohort selection and clinical trial readiness. Whether you're curious about using LLMs in production, building scalable NLP systems, or applying AI to healthcare, this session will offer practical insights, architecture tips, and real-world lessons from a solution that’s already delivering impact.",
      "es": "En esta charla compartiremos cómo construimos y desplegamos un sistema de RAG (Retrieval-Augmented Generation) de nivel productivo usando Python para extraer datos clínicos de documentos médicos no estructurados, permitiendo a los equipos de salud mapear pacientes, obtener insights en tiempo real y acelerar el reclutamiento para ensayos clínicos. Recorreremos la arquitectura de punta a punta del sistema, que procesa más de 4 millones de documentos al mes y combina parsing, clasificación de texto, chunking, embeddings, búsqueda vectorial y extracción de datos con LLMs. Verás cómo Python integra todo en un pipeline modular sobre AWS (Lambda, Step Functions, S3), diseñado para escalabilidad, trazabilidad y eficiencia. La charla destacará: Cómo orquestamos un flujo de procesamiento de documentos de alto rendimiento en Python. Los retos de desplegar LLMs en entornos clínicos (alucinaciones, evaluación, diseño de prompts). Lecciones aprendidas construyendo un RAG personalizado para casos de salud. Cómo la solución permite selección dinámica de cohortes y preparación para ensayos clínicos. Si tienes curiosidad por usar LLMs en producción, construir sistemas NLP escalables o aplicar IA a la salud, esta sesión te dará insights prácticos, tips de arquitectura y lecciones reales de una solución que ya está generando impacto."
    },
    "summary": { "en": "", "es": "" },
    "tags": [
      "Artificial intelligence",
      "Data Science",
      "Computer Vision",
      "Core Python",
      "Devops",
      "Machine Learning"
    ],
    "level": "all",
    "video_url": ""
  },
  {
    "id": 22,
    "title": {
      "en": "Fine-tuning with LLMs: Control and Privacy in Financial NLP",
      "es": "Fine-tuning con LLMs: control y privacidad en NLP financiero"
    },
    "speakers": ["andres-gonzales", "jonny-jimenez"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "In the era of Generative AI (GenAI), language models have revolutionized how we process text. However, in enterprise environments like fintechs and regulated sectors, data generation can compromise model governance, quality, and privacy. In this talk, we'll explore how to use GenAI for labeled data generation and how fine-tuning Transformers allows us to maintain control over model quality and accuracy. Based on a real use case in financial transaction categorization for Mercado Pago in Argentina, we'll see how to train models tailored to specific needs without losing interpretability or reliability. Attendees will learn: How to generate training data with GenAI in a structured way. Strategies for fine-tuning Transformers in Spanish. Metrics and techniques to evaluate accuracy without compromising governance. This talk is ideal for data scientists, ML engineers, and NLP enthusiasts who want to apply language models without losing control over the data.",
      "es": "En la era de la Inteligencia Artificial Generativa (GenAI), los modelos de lenguaje han revolucionado la forma en que procesamos texto. Sin embargo, en entornos empresariales como fintechs y sectores regulados, la generación de datos puede comprometer la gobernabilidad, calidad y privacidad de los modelos. En esta charla, exploraremos cómo usar GenAI para la generación de datos etiquetados y cómo el fine-tuning de Transformers nos permite mantener control sobre la calidad y precisión del modelo. A partir de un caso de uso real en la categorización de transacciones financieras de Mercado Pago en Argentina, veremos cómo entrenar modelos ajustados a necesidades específicas sin perder interpretabilidad ni confiabilidad. Los asistentes aprenderán: ✅ Cómo generar datos de entrenamiento con GenAI de manera estructurada. ✅ Estrategias para el fine-tuning de Transformers en español. ✅ Métricas y técnicas para evaluar la precisión sin comprometer la gobernabilidad. Esta charla es ideal para científicos de datos, ingenieros de ML y entusiastas de NLP que buscan aplicar modelos de lenguaje sin perder control sobre los datos."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Artificial intelligence", "Data Science", "Machine Learning"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 23,
    "title": {
      "en": "Research and implementation of a digital twin for urban agriculture using Python and microservices",
      "es": "Investigación e implementación de un gemelo digital para la agricultura urbana usando Python y microservicios"
    },
    "speakers": ["diego-ramirez", "vivian-castro"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "The Center for Electricity, Electronics, and Telecommunications of the National Learning Service (SENA), aligned with the National Development Plan and aware of its social commitment and the great potential that Industry 4.0 offers in various technological fields, seeks to contribute its installed capacity—represented by equipment and human talent—to mitigating issues related to the Colombian agricultural sector. It has identified an opportunity to enhance harvest decision-making and improve agricultural yields through the adoption of Industry 4.0 technologies. Globally, agriculture faces a series of uncertainties and challenges. According to the FAO, agriculture must become more productive to feed an ever-growing global population while addressing the formidable environmental challenges ahead (FAO, 2023). Climate change is one of the greatest barriers to agricultural development, both rural and urban. The three key pillars to combat it are adaptation, resilience, and mitigation, alongside the adoption of new technological knowledge. For this reason, the efforts of the academic community must focus on making agriculture increasingly efficient, productive, and sustainable. This must be accompanied by the implementation of public policies aimed at strengthening the sector. In Colombia, agriculture has not fully benefited from the technological advancements offered by Industry 4.0. Technologies such as digital twins, virtual reality systems, augmented reality, and data science powered by artificial intelligence can help understand the patterns and dynamics of nature. This deeper understanding aids in better decision-making, ultimately leading to increased agricultural productivity.",
      "es": "El Centro de Electricidad, Electrónica y Telecomunicaciones del SENA, alineado con el Plan Nacional de Desarrollo y consciente de su compromiso social y del gran potencial que la Industria 4.0 ofrece en diversos campos tecnológicos, busca aportar su capacidad instalada—representada en equipos y talento humano—a mitigar problemáticas del sector agrícola colombiano. Ha identificado una oportunidad para mejorar la toma de decisiones de cosecha y aumentar los rendimientos agrícolas mediante la adopción de tecnologías de la Industria 4.0. A nivel global, la agricultura enfrenta una serie de incertidumbres y desafíos. Según la FAO, la agricultura debe volverse más productiva para alimentar a una población mundial en constante crecimiento, enfrentando a la vez los formidables retos ambientales que se avecinan (FAO, 2023). El cambio climático es una de las mayores barreras para el desarrollo agrícola, tanto rural como urbano. Los tres pilares clave para combatirlo son la adaptación, la resiliencia y la mitigación, junto con la adopción de nuevo conocimiento tecnológico. Por ello, los esfuerzos de la academia deben enfocarse en hacer la agricultura cada vez más eficiente, productiva y sostenible, acompañados de la implementación de políticas públicas que fortalezcan el sector. En Colombia, la agricultura no ha aprovechado plenamente los avances tecnológicos que ofrece la Industria 4.0. Tecnologías como los gemelos digitales, los sistemas de realidad virtual, la realidad aumentada y la ciencia de datos impulsada por inteligencia artificial pueden ayudar a comprender los patrones y dinámicas de la naturaleza. Este entendimiento más profundo contribuye a una mejor toma de decisiones, lo que finalmente se traduce en una mayor productividad agrícola."
    },
    "summary": { "en": "", "es": "" },
    "tags": [
      "Artificial intelligence",
      "Data Science",
      "Core Python",
      "Devops",
      "IoT",
      "Machine Learning",
      "Video games"
    ],
    "level": "beginner",
    "video_url": ""
  },
  {
    "id": 24,
    "title": {
      "en": "Asynchrony and Concurrency in Python in 2025",
      "es": "El asincronismo y la concurrencia en Python en 2025"
    },
    "speakers": ["andrew-pikul", "david-ruiz"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "Essential packages: aiohttp, aiofile<br />The foundations of asyncio: coroutines, tasks, and futures<br />Architectures beyond the web: eg. parallel processing<br /><br />New in python 3.10, 3.11, 3.12<br />TaskGroup vs Gather<br />New types of errors and error groups<br />try/await improvements<br />The GIL: why threads don't matter...<br />Blocking calls: why threads do matter<br /><br />PEP 703: The death of the GIL, long live the GIL! Why threads will matter more than ever<br />(Essential packages)<br />-  Types of coroutines and awaitables: native asyncio vs. trio(Types of coroutines / awaitables)<br />-  Architectures beyond the web: parallel processing (Python 3.10, 3.11, 3.12)(Architectures beyond the web: parallel processing)<br />-  New types of errors and error groups: `try/await` (3.10, 3.11, 3.12)(New types of errors and error groups)<br />-  The GIL: why threads don't matter...(GIL: why threads don't matter)<br />-  Blocking calls: why threads do matter (Blocking calls: why threads do matter)<br />-  PEP 703: The death of the GIL, long live the GIL! Why threads will matter more than ever (The death of the GIL, long live the GIL!)",
      "es": "Los paquetes esenciales: aiohttp, aiofile<br />Los cimientos de asyncio: corrutinas, tareas y futuros<br />Arquitecturas más que la web: eg. procesamiento paralelo<br /><br />Lo nuevo en python 3.10, 3.11, 3.12<br />TaskGroup vs Gather<br />Errores nuevos y grupos de errores<br />Mejoras en try/await<br />El GIL: No importan los hilos<br />Llamadas que bloquean: O importan los hilos<br /><br />PEP 703: La muerte del GIL, larga vida al GIL!<br />Porque los hilos importarán cada vez más<br />(Essential packages)<br />-  Tipos de corrutinas y objetos esperables: asyncio nativo vs. trio(Types of coroutines / awaitables)<br />-  Arquitecturas más allá de la web: procesamiento en paralelo (Python 3.10, 3.11, 3.12)(Architectures beyond the web: parallel processing)<br />-  Nuevos tipos de errores y grupos de errores: `try/await` (3.10, 3.11, 3.12)(New types of errors and error groups)<br />-  El GIL: por qué los hilos no importan...(GIL: why threads don't matter)<br />-  Llamadas bloqueantes: por qué los hilos sí importan (Blocking calls: why threads matter)<br />-  PEP 703: ¡Muere el GIL, larga vida al GIL!: por qué los hilos importarán aún más (The death of the GIL, long live the GIL!)"
    },
    "summary": { "en": "", "es": "" },
    "tags": [
      "Core Python",
      "Concurrency",
      "GIL",
      "High-Performance Computing",
      "Parallel Computing"
    ],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 25,
    "title": {
      "en": "LLM-based Automation for Parent Management Training–Oregon Model: A Practical Case in a Chilean Foundation",
      "es": "Automatización basada en LLMs para el Parent Management Training–Oregon Model: un caso práctico en una fundación chilena"
    },
    "speakers": ["karol-sandoval", "daniel-salazar"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "In this talk, we present how, with the Python ecosystem, we designed an end-to-end flow to automate the evaluation of the Parent Management Training–Oregon (PMTO) program. We aimed to test whether language models could rigorously and consistently grade sessions currently analyzed by people, freeing them for strategic tasks that enhance the program. We worked closely with facilitators: gathering hours of audio and their official rubrics to create a solid reference base. After evaluating several AI alternatives, we adopted GPT-4o in Azure and integrated it into a compact pipeline: we transcribed audios with Azure Cognitive Services Speech (including diarization and timestamps); cleaned and exported results with pandas; split them with a TextSplitter on LangChain; and sent each chunk to GPT-4o via AzureChatOpenAI, validating the output with a Pydantic schema that returns only grad_score and summary. All this is exposed in a Streamlit interface that, in seconds, shows the score by thematic axis and the step-by-step justification. The pilot test achieved over 90% agreement with human evaluation, demonstrating the feasibility of reducing review hours and variability among evaluators. During the session, we will break down the architecture, design decisions, and quality metrics, and share lessons the Python community can apply to bring AI to other training environments where objectivity and efficiency are critical.",
      "es": "En esta charla presentaremos cómo, con el ecosistema Python, diseñamos un flujo end‑to‑end para automatizar la evaluación del programa Parent Management Training – Oregon (PMTO). Buscamos probar si los modelos de lenguaje podían calificar—con rigor y consistencia—las sesiones que hoy analizan personas, liberándolas para tareas estratégicas que potencien el programa. Trabajamos codo a codo con los facilitadores: reunimos horas de audio y sus rúbricas oficiales para crear una base de referencia sólida. Tras evaluar varias alternativas de IA, adoptamos GPT‑4o en Azure y lo integramos en un pipeline compacto: transcribimos los audios con Azure Cognitive Services Speech (diarización y marcas de tiempo incluidas); limpiamos y exportamos resultados con pandas; los fragmentamos con un TextSplitter sobre LangChain; y enviamos cada trozo a GPT‑4o vía AzureChatOpenAI, validando la salida con un esquema Pydantic que devuelve únicamente grad_score y resumen. Todo esto se expone en una interfaz Streamlit que, en segundos, muestra la nota por eje temático y la justificación paso a paso. La prueba piloto alcanzó más de 90% de concordancia con la evaluación humana, demostrando la viabilidad de reducir horas de revisión y la variabilidad entre evaluadores. Durante la sesión desglosaremos la arquitectura, las decisiones de diseño y las métricas de calidad, y compartiremos lecciones que la comunidad Python puede aplicar para llevar la IA a otros entornos de capacitación donde objetividad y eficiencia resultan críticas."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Artificial intelligence", "Data Science", "Machine Learning"],
    "level": "all",
    "video_url": ""
  },
  {
    "id": 26,
    "title": {
      "en": "From Notebook to Cloud: Developing Reproducible ML Applications",
      "es": "Del Notebook a la nube: desarrollando aplicaciones reproducibles de ML"
    },
    "speakers": ["david-espejo"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "In this session, we will learn how to use the Python SDK of Flyte, the open-source ML orchestrator, to develop an ML pipeline from a Notebook, and easily determine the inputs used to produce a specific result or return to a particular version of the model, enabling rapid iterations and a scalable step between experimentation and a production ML application.",
      "es": "En esta sesión aprenderemos como usar el SDK Python de Flyte, el orquestador ML de código abierto, para desarrollar un pipeline de ML desde un Notebook, y poder fácilmente determinar las entradas que se usaron para producir un resultado especifico o retornar a una versión particular del modelo, habilitando iteraciones rápidas y un paso escalable entre la experimentación y una aplicación de ML en producción."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["ml", "cloud", "flyte", "python"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 27,
    "title": {
      "en": "Scalability Without Complexity: Clean Architectures with Python",
      "es": "Escalabilidad sin complejidad: Arquitecturas limpias con Python"
    },
    "speakers": ["juan-gomez"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "In this talk, we will explore how to apply clean architecture principles in Python projects to achieve scalable, maintainable, and easy-to-test systems. Using real examples from the Colombian tech environment, we will show how to separate concerns, avoid unnecessary dependencies, and prepare your software to grow without becoming unmanageable.",
      "es": "En esta charla exploraremos cómo aplicar principios de arquitectura limpia en proyectos Python para lograr sistemas escalables, mantenibles y fáciles de probar. Usando ejemplos reales del entorno tecnológico en Colombia, mostraremos cómo separar preocupaciones, evitar dependencias innecesarias y preparar tu software para crecer sin volverse inmanejable."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["architecture", "scalability", "python", "clean architecture"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 28,
    "title": {
      "en": "How to Create Agents with PydanticAI: Simple, Fast, Typed",
      "es": "Cómo crear agentes con PydanticAI: Simple, rápido, tipado"
    },
    "speakers": ["juan-duque"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "The goal of this talk is to encourage attendees to use pydanticai to implement their AI agents. Just as FastAPI has been successful by relying on Pydantic, I see a great opportunity for PydanticAI, which addresses a pressing need: to provide a simple framework that offers a great developer experience. PydanticAI has it all: open source, reputable developers, compatibility with a wide variety of LLMs, and best of all: it's typed!",
      "es": "El objetivo de la charla es incentivar a los asistentes a usar pydanticai para implementar sus agentes de inteligencia artificial. Así como Fastapi ha sido exitoso por apoyarse en Pydantic, vislumbro una gran oportunidad para PydanticAI, el cual atiende una necesidad apremiante que es plantear un framework de trabajo simple y que ofrezca una gran experiencia para el desarrollador. PydanticAI lo tiene todo: open source, sus desarrolladores gozan de gran reputación, compatibilidad con una amplia variedad de LLMs, y lo mejor: es tipado!"
    },
    "summary": { "en": "", "es": "" },
    "tags": ["pydanticai", "agents", "python", "llm"],
    "level": "beginner",
    "video_url": ""
  },
  {
    "id": 29,
    "title": {
      "en": "Python: The New Language of Biological Sciences",
      "es": "Python el nuevo lenguaje de las ciencias biológicas"
    },
    "speakers": ["jennifer-velez"],
    "spoken_language": "spanish",
    "submission": "talk",
    "description": {
      "en": "Currently, DNA and RNA sequencing technologies have enabled the development of biological sciences and the emergence of new disciplines such as omics sciences, which are responsible for identifying, describing, and quantifying biomolecules and their relationships within organisms. Given this situation, contemporary biology faces a real challenge in analyzing and processing the enormous amounts of data obtained. This has led to the need for high computational capacity and the use of programming languages that allow handling and analyzing such information, also including artificial intelligence like Alphafold2 (Nobel Prize in Chemistry) for protein structure prediction and EVO2 to analyze millions of DNA sequences and even generate complete genomes, which is vital for the development of synthetic biology. Both AIs are publicly available and written in Python. This allows for highly accurate simulations before conducting an experiment (enabling the development of more precise and accurate drugs and vaccines, saving time and money in their development) or predicting the function of an altered protein and its effect on disease.",
      "es": "Actualmente las tecnologías de secuenciación de ADN y ARN han permitido el desarrollo de las ciencias biológicas y abriendo nuevas disciplinas como las ciencias ómicas encargada de estudiar de identificar, describir y cuantificar las biomoléculas y su relación dentro de los organismos. Dada esta situación la biología contemporánea encuentra un verdadero reto para analizar y procesar las enormes cantidades de datos obtenidos. Esto ha llevado a que se requieran de altas capacidades de computación y la necesidad de utilizar lenguajes de programación que permitan manejar y análizar dicha información, también incluyendo inteligencia artificial como Alphahold2 (Premio novel de química) para la la predicción de la estructura de las proteínas y EVO2 para analizar millones de secuencias de ADN y generar incluso genomas completos que es vital para el desarrollo de la biología sintetica. Ambos inteligencias artificiales están dispononibles al publico en general y están escritas en Python. Lo que permite generar simulaciones muy acertadas antes de llevar a cabo un experimento (esto permite el desarrollo de medicamentos y vacunas más exactas y precisas ahorrando tiempo y dinero para el desarrollo de las mismas) o predecir la función de una proteína alterada y su efecto en la enfermedad. "
    },
    "summary": { "en": "", "es": "" },
    "tags": ["biology", "omics", "python", "ai"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 30,
    "title": {
      "en": "Wagtail: Create Agile Marketplaces with Django",
      "es": "Wagtail: Crea Marketplaces Ágiles con Django"
    },
    "speakers": ["nieng-yordan"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "es": "Wagtail para Marketplaces: Gestión Ágil de Contenido y Transacciones\n\nEn esta charla, exploraremos cómo Wagtail, el potente CMF de Django, puede ser la base ideal para construir marketplaces dinámicos y administrables sin necesidad de modificar el código fuente. Basado en mi experiencia desarrollando una aplicación real, compartiré cómo integré tecnologías como jQuery, JavaScript, Docker y Dokku para desplegar una plataforma robusta y escalable.\n\nEsta aplicación permite a empresas con alto impacto ambiental compensar su huella de carbono mediante la compra de créditos. Integra pasarelas de pago como OpenPay, facturación electrónica con Contapyme, y gestión de créditos de carbono con Biocarbon. Además, utiliza PostgreSQL para bases de datos y Celery para tareas automatizadas, como la actualización de pagos, generación de reportes y emisión de certificados en PDF con WeasyPrint.\n\nEn esta sesión, los asistentes aprenderán:\n✅ Cómo usar Wagtail para construir marketplaces con contenido administrable.\n✅ Integraciones clave: pasarelas de pago, facturación electrónica y automatización de procesos.\n✅ Despliegue eficiente con Docker y Dokku en Digital Ocean.\n✅ Casos de uso reales donde la tecnología ayuda a la sostenibilidad ambiental.\n\nSi buscas desarrollar plataformas flexibles, optimizar procesos empresariales y generar impacto positivo, ¡esta charla es para ti!",
      "en": "Wagtail for Marketplaces: Agile Content and Transaction Management\n\nIn this talk, we’ll explore how Wagtail, the powerful CMF built on Django, can serve as the ideal foundation to build dynamic, maintainable marketplaces—without modifying the source code. Based on my experience building a real-world application, I’ll share how I integrated technologies like jQuery, JavaScript, Docker, and Dokku to deploy a robust, scalable platform.\n\nThis platform allows companies with high environmental impact to offset their carbon footprint by purchasing credits. It integrates payment gateways like OpenPay, electronic invoicing with Contapyme, and carbon credit management with Biocarbon. It also uses PostgreSQL for databases and Celery for automated tasks like payment updates, report generation, and PDF certificate issuance using WeasyPrint.\n\nIn this session, attendees will learn:\n✅ How to use Wagtail to build marketplaces with editable content.\n✅ Key integrations: payment gateways, electronic invoicing, and process automation.\n✅ Efficient deployment with Docker and Dokku on Digital Ocean.\n✅ Real use cases where technology supports environmental sustainability.\n\nIf you’re looking to develop flexible platforms, optimize business processes, and create positive impact, this talk is for you!"
    },
    "summary": { "en": "", "es": "" },
    "tags": ["core python", "devops", "web"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 31,
    "title": {
      "en": "MicroPython for IoT with LoRa: Low Power Sensor Networks",
      "es": "MicroPython para IoT con LoRa: Redes de Sensores de Bajo Consumo"
    },
    "speakers": ["cristian-valdez-romero"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "es": "Este taller tiene como objetivo principal enseñar a los asistentes cómo utilizar MicroPython para desarrollar aplicaciones IoT que usen LoRa para la comunicación de largo alcance con bajo consumo energético.\n\nDurante el desarrollo del taller se desarrollará la siguiente temática:\n\n- Introducción a LoRa y LoRaWAN\n- Diferencias entre LoRa y LoRaWAN.\n- Ventajas y casos de uso en IoT.\n- Instalación de MicroPython en microcontroladores\n- Instalación en ESP32, ESP8266 y Raspberry Pi Pico W.\n- Configuración de herramientas (Thonny, ampy, esptool, etc.).\n- Configuración básica de módulos LoRa.\n- Envío y recepción de datos entre dispositivos usando MicroPython.\n- Red de Sensores con LoRa y MicroPython\n- Lectura de sensores.\n- Transmisión de datos mediante LoRa.\n- Integración con Internet y Aplicaciones IoT\n- Conectar un Gateway LoRa a Internet.\n- Enviar datos a una API o plataforma en la nube.\n- Optimización de Consumo Energético en IoT con LoRa\n- Modos de bajo consumo en ESP32/ESP8266/Rpi Pico.\n- Estrategias para reducir el consumo energético en sensores y nodos LoRa.\n- Integración con una aplicación web o MQTT para mostrar datos.\n- Desafío Final: Implementando un Sistema IoT Completo\n\nCon el fin de desarrollar el taller pretendo llevar varios módulos LoRa, sensores y microcontroladores como Rpi Pico, Esp32, Esp8266, EasyRobot y Motika, protoboard, etc. con el fin de crear un entorno interactivo en el cual se pueda verificar en tiempo real el funcionamiento y el propósito de cada uno de los componentes a utilizar durante el workshop.",
      "en": "This workshop aims to teach attendees how to use MicroPython to develop IoT applications that utilize LoRa for long-range, low-power communication.\n\nThe workshop will cover the following topics:\n\n- Introduction to LoRa and LoRaWAN\n- Differences between LoRa and LoRaWAN\n- Advantages and IoT use cases\n- Installing MicroPython on microcontrollers\n- Installation on ESP32, ESP8266, and Raspberry Pi Pico W\n- Tool setup (Thonny, ampy, esptool, etc.)\n- Basic configuration of LoRa modules\n- Sending and receiving data between devices using MicroPython\n- Sensor Networks with LoRa and MicroPython\n- Reading data from sensors\n- Transmitting data via LoRa\n- Internet and IoT App Integration\n- Connecting a LoRa Gateway to the Internet\n- Sending data to an API or cloud platform\n- Power Optimization in IoT with LoRa\n- Low-power modes for ESP32/ESP8266/RPi Pico\n- Strategies to reduce energy consumption in sensors and LoRa nodes\n- Integration with a web app or MQTT to display data\n- Final Challenge: Building a Complete IoT System\n\nTo support this hands-on workshop, I plan to bring various LoRa modules, sensors, and microcontrollers such as RPi Pico, ESP32, ESP8266, EasyRobot, and Motika, along with breadboards and other components. The goal is to create an interactive environment where participants can see each component’s functionality and purpose in real time."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["iot", "micropython"],
    "level": "all",
    "video_url": ""
  },
  {
    "id": 32,
    "title": {
      "en": "Dashboard from Scratch: Connecting to Databases and Visualizing with Python",
      "es": "Cuadro de mando desde cero: Conexión a bases de datos y visualización con Python"
    },
    "speakers": ["erick-pineda-amezquita"],
    "spoken_language": "english",
    "submission": "workshop",
    "description": {
      "es": "Este taller ofrece una introducción práctica al desarrollo de dashboards web interactivos utilizando Python.\n\nLos participantes aprenderán a crear visualizaciones dinámicas, conectar bases de datos para consultas en tiempo real y desplegar sus aplicaciones en el navegador.\n\nIdeal para quienes desean transformar datos en insights visuales accesibles desde cualquier dispositivo.\n\n· ¿Qué es un dashboard web y para qué sirve?\n· Principales librerías para dashboards.\n· Comparación con otras herramientas comerciales.\n· Instalación de las herramientas.\n· Configuración del entorno de trabajo.\n· Crear un dashboard básico.\n· Introducción a la conexión con bases de datos.\n· Consultas simples desde Python.\n\nSe recomienda traer laptop.",
      "en": "This workshop offers a hands-on introduction to developing interactive web dashboards using Python.\n\nParticipants will learn how to build dynamic visualizations, connect to databases for real-time queries, and deploy their applications in the browser.\n\nIt’s ideal for those who want to turn data into accessible visual insights from any device.\n\n· What is a web dashboard and what is it used for?\n· Main libraries for dashboards.\n· Comparison with other commercial tools.\n· Installing the required tools.\n· Setting up the development environment.\n· Building a basic dashboard.\n· Introduction to database connections.\n· Simple queries from Python.\n\nBringing a laptop is recommended."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["data science", "business intelligence", "data visualization"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 33,
    "title": {
      "en": "A hacker’s guide for geospatial analysis using python and qgis",
      "es": "Guía del hacker para el análisis geoespacial con python y qgis"
    },
    "speakers": ["jorge-martinez"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "en": "Within the geospatial world, we are usually immersed with different analysis for specific purposes. It ranges from basic intersection of polygons, to complex estimations such as population at risk to floods in Sri Lanka. These operations involve a defined set of inputs and a set of steps or instructions, whose number depends on the final goal. Using previous examples, intersection requires two polygons, while the flood analysis needs a population and flood layers, country administrative boundaries, etc. From a hacker’s perspective, as we break down what we can denote as workflow, into a minimum set of commands, we realize that executing them in a manual manner could be overwhelming, since the number of operations increases. Furthermore, as a hacker you become aware that there might exist human-prone errors, such as incorrect inputs or parameters.\n\nAs an alternative to performing analysis manually, we can write a script with the same set of instructions. We realize that the big goal can be automated, reducing time and manual mistakes. Also, we become aware that this workflow scales to multiple inputs. For instance, the flood analysis can be the same for Sri Lanka or Colombia, what changes is the country boundaries. Also, we can gain more precision using city information rather than district. Finally, we can extend the workflow by adding building polygons or road networks.\n\nBased on previous paragraphs, we can see the benefits of an automated workflow using scripts. We can enhance current software available, such as QGIS, to create, modify and share geospatial workflows in an easy-to-run way. We can leverage the current toolbox by writing custom scripts to perform specific tasks that are not existing within it. Using a hacker’s mindset, it will be possible to design tasks and execute instructions by modifying what is available to fulfill the milestone.\n\nWith this workshop, I want to share the knowledge for a successful geospatial analysis and automation workflow, commonly used within an emergency context as an example, but with the ability to be extended into multiple domains where geospatial data is used. Using QGIS designer, we will create a graph that, by receiving specific inputs, will run commands to measure the amount of people exposed to earthquakes or floods, and combine the results with vulnerability indicators, like poverty or food security indexes. I will also explain how to make use of QGIS Python runtime to create custom functions to be further connected into the graph.\n\nThe contributions of this workshop are twofold:\n- Attendees will create an automated geospatial analysis, increasing productivity while reducing manual errors. All of this encompassed within QGIS Python runtime, which will allow shareability without dependency concerns or issues.\n- We can extend the QGIS processing toolbox by creating scripts that run specific tasks. It will be possible to insert them into a processing graph for a fully automated workflow.\n\nSchedule:\n- Introduction to geospatial data and QGIS (25 minutes).\n- Simple automation for a population exposure analysis (40 minutes).\n- Input/Output definitions, simple tasks and execution runtime (1 hour)\n- Extension of processing toolbox via python scripting. Development of a processing script to download data. (1 hour,15 minutes).\n- Model export and shareability (20 minutes)",
      "es": "En el mundo geoespacial, a menudo realizamos distintos análisis para propósitos específicos. Estos van desde la intersección básica de polígonos, hasta estimaciones complejas como la población en riesgo por inundaciones en Sri Lanka. Estas operaciones requieren un conjunto definido de entradas y una serie de pasos o instrucciones, cuyo número depende del objetivo final. Por ejemplo, la intersección necesita dos polígonos, mientras que el análisis de inundaciones requiere capas de población e inundación, límites administrativos del país, etc. Desde una mentalidad hacker, al descomponer este flujo de trabajo en comandos mínimos, nos damos cuenta de que ejecutarlos manualmente puede ser abrumador, especialmente si el número de operaciones es alto. Además, existen riesgos de errores humanos como entradas o parámetros incorrectos.\n\nComo alternativa al análisis manual, podemos escribir un script con las mismas instrucciones. Esto permite automatizar el objetivo general, reduciendo tiempo y errores. Este enfoque también escala con múltiples entradas. Por ejemplo, el análisis de inundaciones puede aplicarse tanto a Sri Lanka como a Colombia, cambiando únicamente los límites del país. Se puede ganar precisión usando información a nivel de ciudad en lugar de distrito. Incluso se puede extender el flujo de trabajo incluyendo capas de edificios o redes viales.\n\nComo vimos, los scripts ofrecen un flujo de trabajo automatizado con múltiples beneficios. Podemos mejorar herramientas existentes como QGIS para crear, modificar y compartir análisis geoespaciales de forma sencilla. Además, podemos ampliar la caja de herramientas desarrollando scripts personalizados para tareas específicas. Con mentalidad hacker, es posible diseñar tareas y modificar lo disponible para lograr el objetivo deseado.\n\nEn este taller quiero compartir el conocimiento necesario para realizar un análisis geoespacial exitoso y un flujo de trabajo automatizado. Aunque nos enfocaremos en contextos de emergencia, los conceptos pueden aplicarse a múltiples dominios donde se use información geoespacial. Usando el diseñador de QGIS, crearemos un grafo que, con ciertas entradas, ejecutará comandos para medir cuántas personas están expuestas a terremotos o inundaciones, combinando los resultados con indicadores de vulnerabilidad como pobreza o seguridad alimentaria. También explicaré cómo usar el entorno de Python en QGIS para crear funciones personalizadas que se integrarán en el grafo.\n\nLas contribuciones del taller son:\n- Los asistentes crearán un análisis geoespacial automatizado, aumentando la productividad y reduciendo errores manuales. Todo dentro del entorno Python de QGIS, lo cual facilita compartir el trabajo sin problemas de dependencias.\n- Extenderemos la caja de herramientas de QGIS mediante scripting en Python para realizar tareas específicas. Estos scripts se podrán integrar en grafos de procesamiento para flujos completamente automatizados.\n\nAgenda:\n- Introducción a los datos geoespaciales y QGIS (25 minutos).\n- Automatización sencilla para un análisis de exposición poblacional (40 minutos).\n- Definición de entradas/salidas, tareas simples y ejecución (1 hora).\n- Ampliación de la caja de herramientas con scripts en Python: desarrollo de un script para descarga de datos (1 hora, 15 minutos).\n- Exportación del modelo y compartición (20 minutos)."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["data science", "scientific computing", "gis"],
    "level": "beginner",
    "video_url": ""
  },
  {
    "id": 34,
    "title": {
      "en": "Your new digital companion: you build it, in this workshop",
      "es": "Tu nuevo compañero digital: lo construyes tú, en este taller"
    },
    "speakers": ["gilbert-alejandro-gomez-briceno"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "es": "En este taller construiremos un asistente personalizado diseñado para facilitar tareas cotidianas. A lo largo del proceso, trabajaremos con herramientas como Python, Chainlit, LangChain, bases de datos e integraciones con APIs públicas. Además, incorporaremos observabilidad mediante Arize Phoenix, lo que nos permitirá visualizar, evaluar y analizar las trazas del comportamiento del asistente.\n\nDurante el taller, desarrollaremos paso a paso las funciones principales del asistente. Una de ellas será el análisis de gastos personales, donde a partir de un archivo, el asistente identificará en qué estamos gastando y propondrá posibles ajustes. También implementaremos una función de resumen y análisis de noticias personalizadas, basada en nuestras preferencias e historial, cuyos resultados se enviarán directamente por correo electrónico. Finalmente, exploraremos su aplicación en la investigación académica, permitiéndole buscar y recopilar artículos científicos en línea para apoyar procesos de estudio e indagación.",
      "en": "In this workshop, we will build a personalized assistant designed to simplify everyday tasks. Throughout the process, we’ll work with tools such as Python, Chainlit, LangChain, databases, and integrations with public APIs. We will also incorporate observability using Arize Phoenix, enabling us to visualize, evaluate, and analyze the assistant's behavior traces.\n\nDuring the workshop, we’ll develop the assistant’s core functions step by step. One of them will be personal expense analysis, where the assistant will process a file, identify spending patterns, and suggest possible adjustments. We will also implement a feature for summarizing and analyzing personalized news, based on our preferences and history, with results sent directly by email. Finally, we’ll explore its use in academic research by enabling the assistant to search for and gather scientific articles online to support study and investigation processes."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["artificial intelligence"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 35,
    "title": {
      "en": "Model-Based Agents: Create Synthetic Data and Intelligent Simulations",
      "es": "Agentes Basados en Modelos: Crea Datos Sintéticos y Simulaciones Inteligentes"
    },
    "speakers": ["catalina-cruz"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "es": "¿Te imaginas poder crear tus propios escenarios de trading o simulaciones de inversiones sin depender de datos reales y restringidos? En este workshop, descubrirás cómo aprovechar el poder de la modelación basada en agentes (ABM) para generar datos sintéticos que reflejen comportamientos de mercado, flujo de órdenes y estrategias de inversión. Aprenderás a diseñar agentes con reglas de interacción y a integrar esos resultados con herramientas de machine learning para entrenar y validar modelos financieros. ¡Ideal para quienes buscan innovar en la investigación y desarrollo de algoritmos de trading, predicciones de mercado o simplemente expandir sus horizontes en ciencia de datos!",
      "en": "Imagine being able to create your own trading scenarios or investment simulations without relying on real, restricted data. In this workshop, you’ll discover how to harness the power of Agent-Based Modeling (ABM) to generate synthetic data that reflects market behavior, order flow, and investment strategies. You’ll learn how to design agents with interaction rules and integrate those results with machine learning tools to train and validate financial models. Perfect for those looking to innovate in algorithmic trading research, market prediction, or simply expand their horizons in data science!"
    },
    "summary": { "en": "", "es": "" },
    "tags": [
      "artificial intelligence",
      "data science",
      "Machine Learning",
      "scientific computing",
      "web"
    ],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 36,
    "title": {
      "en": "Build Serverless APIs with Python and AWS",
      "es": "Construye APIs Serverless con Python y AWS"
    },
    "speakers": ["christian-abrajan-fortis"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "es": "¿Quieres desarrollar APIs escalables, seguras y sin preocuparte de los servidores? En este workshop 100% hands-on, construiremos paso a paso una API moderna usando:\n- Python + Serverless Framework (infraestructura como código)\n- AWS Lambda & API Gateway (backend sin servidores)\n- DynamoDB (base de datos serverless)\n- Autenticación (Cognito o API Keys)\n🚀 Despliegue automático con CI/CD básico\n\nQué haremos:\n1. Configuración inicial del entorno (pre-requisitos guiados)\n2. Desarrollo local de una API REST/HTTP con Python\n3. Integración con servicios AWS desde el código\n4. Despliegue en la nube con un solo comando\n5. Discusión de arquitecturas avanzadas y optimización\n\nLlévate:\n- Código base funcional desde el repositorio.\n- Plantilla reutilizable para tus proyectos.\n- Checklist de mejores prácticas\n\nPerfecto para:\nDevs que quieren entrar al mundo serverless o mejorar sus skills en la nube",
      "en": "Want to build scalable, secure APIs without worrying about servers? In this 100% hands-on workshop, we’ll build a modern API step by step using:\n- Python + Serverless Framework (infrastructure as code)\n- AWS Lambda & API Gateway (serverless backend)\n- DynamoDB (serverless database)\n- Authentication (Cognito or API Keys)\n🚀 Automatic deployment with basic CI/CD\n\nWhat we’ll do:\n1. Initial setup and guided prerequisites\n2. Local development of a REST/HTTP API using Python\n3. Integration with AWS services from the code\n4. Cloud deployment with a single command\n5. Discussion of advanced architectures and optimization\n\nYou’ll leave with:\n- Functional base code from the repository\n- A reusable template for your projects\n- A checklist of best practices\n\nPerfect for:\nDevelopers who want to dive into serverless or level up their cloud skills"
    },
    "summary": { "en": "", "es": "" },
    "tags": ["devops", "web", "serverless"],
    "level": "all",
    "video_url": ""
  },
  {
    "id": 37,
    "title": {
      "en": "Sktime: the unified framework for timeseries forecasting",
      "es": "Sktime: el marco unificado para la previsión de series temporales"
    },
    "speakers": ["felipe-angelim-vieira"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "es": "Este workshop abordará desde los conceptos básicos del framework hasta funcionalidades más avanzadas. Exploraremos el uso de modelos simples y haremos una revisión general del framework. También mostraremos cómo realizar pronósticos probabilísticos, además de utilizar modelos globales y métodos de reconciliación jerárquica, muy relevantes en aplicaciones reales.\n\nEl tutorial contará con la presencia de un desarrollador principal (core-dev) de la biblioteca, siendo una excelente oportunidad para que nuevos miembros de la comunidad open-source comprendan el funcionamiento del framework y cómo contribuir.",
      "en": "This workshop will cover everything from the basic concepts of the framework to more advanced features. We’ll explore the use of simple models and provide a general overview of the framework. We will also demonstrate how to make probabilistic forecasts, use global models, and apply hierarchical reconciliation methods—highly relevant in real-world applications.\n\nThe tutorial will be led by a core developer of the library, making it a great opportunity for new members of the open-source community to understand how the framework works and how to contribute."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["artificial intelligence", "data science", "Machine Learning"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 38,
    "title": {
      "en": "Local AI Lab: Running models with Python and Docker without complications",
      "es": "Laboratorio local de IA: Corre modelos con Python y Docker sin complicaciones"
    },
    "speakers": ["maris-botero"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "es": "¿Te gustaría ejecutar modelos de inteligencia artificial con una sola línea de código, sin preocuparte por entornos, dependencias o configuraciones complicadas? En este taller práctico construiremos un laboratorio de desarrollo con Python que no solo incluye scripts y APIs, sino también ¡modelos de IA listos para correr!\n\nExploraremos la nueva funcionalidad de docker run --model, que permite ejecutar modelos como Llama 2, Mistral o Gemma directamente desde la terminal, sin necesidad de instalar librerías pesadas. A partir de ahí, aprenderás cómo integrar estos modelos en flujos de trabajo con Python, crear pequeñas APIs y experimentar con proyectos de ciencia de datos, todo desde un entorno limpio, portátil y reproducible.\n\nEste taller es ideal para personas que quieren comenzar en el mundo de la IA generativa desde Python, entender cómo funciona el despliegue local de modelos, y llevarse un entorno práctico que pueden adaptar a sus propios proyectos.",
      "en": "Would you like to run artificial intelligence models with a single line of code, without worrying about environments, dependencies, or complicated setups? In this hands-on workshop, we’ll build a Python development lab that includes not only scripts and APIs but also ready-to-run AI models!\n\nWe’ll explore the new `docker run --model` feature, which lets you run models like Llama 2, Mistral, or Gemma directly from the terminal—no need to install heavy libraries. From there, you'll learn how to integrate these models into Python workflows, build small APIs, and experiment with data science projects, all from a clean, portable, and reproducible environment.\n\nThis workshop is perfect for people who want to get started with generative AI in Python, understand how to run models locally, and take home a practical setup they can use in their own projects."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["artificial intelligence", "data science", "devops"],
    "level": "beginner",
    "video_url": ""
  },
  {
    "id": 39,
    "title": {
      "en": "A Deep Dive into Web Performance Testing: Playwright vs. Locust",
      "es": "Una inmersión profunda en las pruebas de rendimiento web: Playwright frente a Locust"
    },
    "speakers": ["rodrigo-silva-ferreira"],
    "spoken_language": "english",
    "submission": "workshop",
    "description": {
      "en": "In this interactive workshop, we’ll explore two powerful tools for web performance testing — Locust, a Python-native load testing framework, and Playwright, a modern browser automation tool. Through guided, hands-on exercises, participants will simulate user load on a real-world site and measure key performance indicators such as response time, error rate, and resource usage.\n\nWe’ll cover:\n- How to write and run basic load tests with Locust.\n- How to script browser-level performance flows in Playwright.\n- How to scale user simulations from 1 to 100 concurrent users.\n- How to interpret test results and understand system bottlenecks.\n- Practical trade-offs: realism, scalability, scripting complexity, and when to use each tool.\n\nBy the end of the session, attendees will walk away with working scripts, visualized metrics, and a clear understanding of how to choose the right tool for their performance testing needs.",
      "es": "En este taller interactivo exploraremos dos herramientas poderosas para pruebas de rendimiento web: Locust, un framework de pruebas de carga nativo de Python, y Playwright, una moderna herramienta de automatización de navegador. A través de ejercicios prácticos guiados, los participantes simularán carga de usuarios sobre un sitio real y medirán indicadores clave de rendimiento como el tiempo de respuesta, la tasa de errores y el uso de recursos.\n\nVeremos:\n- Cómo escribir y ejecutar pruebas básicas de carga con Locust.\n- Cómo crear flujos de rendimiento a nivel de navegador con Playwright.\n- Cómo escalar simulaciones de 1 a 100 usuarios concurrentes.\n- Cómo interpretar los resultados y entender cuellos de botella en el sistema.\n- Consideraciones prácticas: realismo, escalabilidad, complejidad de los scripts y cuándo usar cada herramienta.\n\nAl final del taller, los asistentes se llevarán scripts funcionales, métricas visualizadas y una comprensión clara de cómo elegir la herramienta adecuada para sus necesidades de pruebas de rendimiento."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["devops", "web", "software testing"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 40,
    "title": {
      "en": "Development of digital twin for urban agriculture using free software",
      "es": "Desarrollo de gemelo digital para agricultura Urbana usando software libre"
    },
    "speakers": ["diego-ramirez", "vivian-castro"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "en": "The Center for Electricity, Electronics, and Telecommunications of the National Learning Service (SENA), aligned with the National Development Plan and aware of its social commitment and the great potential that Industry 4.0 offers in various technological fields, seeks to contribute its installed capacity—represented by equipment and human talent—to mitigating issues related to the Colombian agricultural sector. It has identified an opportunity to enhance harvest decision-making and improve agricultural yields through the adoption of Industry 4.0 technologies.\n\nGlobally, agriculture faces a series of uncertainties and challenges. According to the FAO, agriculture must become more productive to feed an ever-growing global population while addressing the formidable environmental challenges ahead (FAO, 2023). Climate change is one of the greatest barriers to agricultural development, both rural and urban. The three key pillars to combat it are adaptation, resilience, and mitigation, alongside the adoption of new technological knowledge.\n\nFor this reason, the efforts of the academic community must focus on making agriculture increasingly efficient, productive, and sustainable. This must be accompanied by the implementation of public policies aimed at strengthening the sector. In Colombia, agriculture has not fully benefited from the technological advancements offered by Industry 4.0. Technologies such as digital twins, virtual reality systems, augmented reality, and data science powered by artificial intelligence can help understand the patterns and dynamics of nature. This deeper understanding aids in better decision-making, ultimately leading to increased agricultural productivity.",
      "es": "El Centro de Electricidad, Electrónica y Telecomunicaciones del Servicio Nacional de Aprendizaje (SENA), alineado con el Plan Nacional de Desarrollo y consciente de su compromiso social y del gran potencial que ofrece la Industria 4.0 en diversos campos tecnológicos, busca aportar su capacidad instalada—representada en equipos y talento humano—para mitigar problemáticas del sector agro colombiano. Se ha identificado una oportunidad para mejorar la toma de decisiones sobre cosechas y aumentar los rendimientos agrícolas mediante la adopción de tecnologías de Industria 4.0.\n\nA nivel mundial, la agricultura enfrenta una serie de incertidumbres y desafíos. Según la FAO, debe volverse más productiva para alimentar a una población mundial en constante crecimiento y, al mismo tiempo, afrontar los enormes desafíos ambientales que se avecinan (FAO, 2023). El cambio climático es una de las mayores barreras para el desarrollo agrícola, tanto rural como urbano. Los tres pilares clave para enfrentarlo son la adaptación, la resiliencia y la mitigación, junto con la adopción de nuevos conocimientos tecnológicos.\n\nPor ello, los esfuerzos de la comunidad académica deben centrarse en hacer que la agricultura sea cada vez más eficiente, productiva y sostenible. Esto debe ir acompañado de la implementación de políticas públicas que fortalezcan el sector. En Colombia, la agricultura no ha aprovechado completamente los avances tecnológicos que ofrece la Industria 4.0. Tecnologías como los gemelos digitales, sistemas de realidad virtual, realidad aumentada y ciencia de datos impulsada por inteligencia artificial pueden ayudar a comprender mejor los patrones y dinámicas de la naturaleza. Esta comprensión más profunda permite tomar mejores decisiones y, en última instancia, aumentar la productividad agrícola."
    },
    "summary": { "en": "", "es": "" },
    "tags": [
      "Artificial intelligence",
      "Data Science",
      "Computer Vision",
      "IoT",
      "Machine Learning",
      "video games"
    ],
    "level": "beginner",
    "video_url": ""
  },
  {
    "id": 41,
    "title": {
      "en": "The data of the game: data analysis in soccer",
      "es": "Los datos del juego: análisis de datos en el fútbol"
    },
    "speakers": ["sebastian-gaviria-giraldo", "jhoan-moscoso"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "es": "Este workshop es una continuación práctica de la charla \"El juego de los datos: análisis de datos en el fútbol” y está diseñado para que los participantes experimenten de forma directa cómo la ciencia de datos, combinada con Python, puede aplicarse al análisis deportivo, específicamente en el fútbol.\n\nA lo largo del taller trabajaremos con datos reales de partidos y jugadores, utilizando herramientas como pandas, matplotlib, seaborn, etc. Aprenderemos a calcular métricas como xG (expected goals), mapas de calor y redes de pases, aplicando conceptos tanto técnicos como tácticos del fútbol.\n\nEl objetivo es que cada asistente construya su propio mini-proyecto de análisis y visualización futbolística, y se lleve las bases para seguir desarrollando herramientas propias, ya sea para scouting, análisis de rendimiento o estrategia de juego.",
      "en": "This workshop is a hands-on continuation of the talk \"The Data Game: Football Data Analysis\" and is designed for participants to directly experience how data science, combined with Python, can be applied to sports analysis, specifically in football.\n\nThroughout the session, we’ll work with real match and player data using tools like pandas, matplotlib, seaborn, and more. Participants will learn to calculate metrics such as xG (expected goals), heatmaps, and passing networks, while applying both technical and tactical football concepts.\n\nThe goal is for each attendee to build their own mini football analysis and visualization project, gaining a solid foundation to continue developing their own tools—whether for scouting, performance analysis, or game strategy."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Data Science", "Community", "Scientific Computing"],
    "level": "all",
    "video_url": ""
  },
  {
    "id": 42,
    "title": {
      "en": "The RAGening: What Retrieval Can Learn from a Certain Animated Town",
      "es": "El RAGening: Lo que la recuperación puede aprender de cierta ciudad animada"
    },
    "speakers": ["nicolas-roldan-fajardo", "maria-fernanda-rojas"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "es": "Los grandes modelos de lenguaje o Large Language Models (LLM) lideran hoy en día una nueva revolución digital con su capacidad para entender el lenguaje, con aplicaciones que ya están generando un impacto en la industria. Pero, ¿qué pasa cuando se necesita información actualizada que no estaba en su conjunto de entrenamiento? Es aquí donde entra en juego la técnica de Retrieval Augmented Generation (RAG), la cuál consiste en traer “chunks” de información relevante a nuestro prompt original proveniente de fuentes externas para luego pasarlas junto a nuestro prompt al LLM.\n\nSin embargo, el RAG tradicional, en su fase de “retrieval”, al estar basado principalmente en búsquedas de similitud vectorial (del prompt con los chunks), se queda corto en ciertas aplicaciones, donde no solo basta pasar “chunks” similares a nuestro prompt dejando de lado las relaciones no semánticas entre los mismos, llevando a respuestas fragmentadas o incompletas.\n\nAquí es donde las bases de conocimiento de grafos entran en escena como una evolución poderosa. GraphRAG utiliza la riqueza de los grafos para representar la información no solo como texto aislado, sino como entidades interconectadas y sus relaciones. Durante el workshop, aplicaremos estos conceptos al universo de Los Simpson, utilizando un dataset que incluye personajes, lugares y diálogos, y veremos cómo preguntas sobre dicho universo pueden resolverse de forma más efectiva.",
      "en": "Large Language Models (LLMs) are leading a new digital revolution thanks to their ability to understand natural language, with real-world applications already impacting many industries. But what happens when updated information is needed—data that wasn’t part of the model's training set? That’s where Retrieval Augmented Generation (RAG) comes into play. RAG retrieves relevant 'chunks' of external information and adds them to the original prompt before passing everything to the LLM.\n\nHowever, traditional RAG, which relies primarily on vector similarity searches between the prompt and the chunks, falls short in some scenarios. It often fails to capture non-semantic relationships between the data, leading to fragmented or incomplete answers.\n\nGraph-based knowledge systems offer a powerful evolution to this method. GraphRAG leverages the richness of graphs to represent information not just as isolated text, but as interconnected entities and their relationships. In this workshop, we’ll apply these concepts using the universe of *The Simpsons*, with a dataset of characters, locations, and dialogues, and explore how questions about this universe can be answered more effectively using graph-based retrieval."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["artificial intelligence", "Machine Learning"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 43,
    "title": {
      "en": "Agentics Solutions: Autonomous Intelligence for Real Challenges",
      "es": "Soluciones Agentics: Inteligencia Autónoma para Retos Reales"
    },
    "speakers": ["yeison-mauricio-munoz"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "es": "🐍⚡ ¿Y si tu código en Python pudiera pensar por sí solo? En este workshop verás agentes que no solo ejecutan scripts, sino que razonan, planean y actúan. Demos en vivo, APIs, lógica autónoma y soluciones que escalan. Todo con código real. #NoEsMagiaEsAgentic",
      "en": "🐍⚡ What if your Python code could think for itself? In this workshop, you'll explore agents that don’t just run scripts—they reason, plan, and act. Live demos, APIs, autonomous logic, and scalable solutions. All built with real code. #It’sNotMagicIt’sAgentic"
    },
    "summary": { "en": "", "es": "" },
    "tags": [
      "Artificial intelligence",
      "Data Science",
      "Machine Learning",
      "LLM - Large Languages Models"
    ],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 44,
    "title": {
      "en": "From Collapse to Performance: Scaling and Managing Massive Data with ScyllaDB",
      "es": "Del colapso al rendimiento: Escalado y gestión de datos masivos con ScyllaDB"
    },
    "speakers": ["diego-vivas-piza"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "en": "In this hands-on, two-hour workshop we’ll simulate—live and in real time—the simultaneous insertion, reading, and updating of thousands of records per minute. Using Python scripts with asyncio, we’ll compare the capabilities of ScyllaDB and PostgreSQL. The workshop will show how ScyllaDB dramatically outperforms PostgreSQL for bulk inserts and queries, backed by real-time metric visualization in Grafana and Prometheus.\n\nWe’ll also cover when you should (and shouldn’t) use ScyllaDB. The session draws inspiration from real-world scaling scenarios—such as the migration story here: https://www.scylladb.com/2024/01/16/migrating-from-postgres-to-scylladb/\n\nParticipants will leave with practical know-how and concrete tools for selecting and configuring ScyllaDB in cases where PostgreSQL quickly hits its limits.",
      "es": "En este taller práctico de dos horas, simularemos —en vivo y en tiempo real— la inserción, lectura y actualización simultánea de miles de registros por minuto. Usando scripts en Python con `asyncio`, compararemos las capacidades de ScyllaDB y PostgreSQL. El taller mostrará cómo ScyllaDB supera de forma contundente a PostgreSQL en inserciones y consultas masivas, respaldado por visualizaciones de métricas en tiempo real usando Grafana y Prometheus.\n\nTambién cubriremos cuándo (y cuándo no) deberías usar ScyllaDB. La sesión se inspira en escenarios reales de escalabilidad, como la historia de migración disponible aquí: https://www.scylladb.com/2024/01/16/migrating-from-postgres-to-scylladb/\n\nLos participantes se llevarán conocimientos prácticos y herramientas concretas para seleccionar y configurar ScyllaDB en casos donde PostgreSQL alcanza rápidamente sus límites."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Core Python", "Web"],
    "level": "all",
    "video_url": ""
  },
  {
    "id": 45,
    "title": {
      "en": "Complexipy: A Deep Dive into Code Readability & Maintainability",
      "es": "Complexipy: Una inmersión profunda en la legibilidad y el mantenimiento del código"
    },
    "speakers": ["robin-hafid-quintero-lopez"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "en": "In this hands-on workshop, we'll go beyond Cyclomatic Complexity and dive into Cognitive Complexity — a modern approach to measuring how difficult code is to understand. You'll learn how to spot problem areas in your codebase, catch complexity early in CI, and refactor code with confidence.\n\nUsing complexipy — a fast, Rust-powered analysis tool — you’ll run real audits, automate quality gates, and integrate complexity feedback right into your editor and pull requests. Whether you're working solo or in a team, you'll walk away with tools and strategies to write cleaner, more maintainable Python code starting today.",
      "es": "En este taller práctico iremos más allá de la complejidad ciclomática y profundizaremos en la complejidad cognitiva: un enfoque moderno para medir qué tan difícil es comprender el código. Aprenderás a identificar áreas problemáticas en tu base de código, detectar la complejidad de forma temprana en CI y refactorizar con confianza.\n\nUsando `complexipy`, una herramienta de análisis rápida impulsada por Rust, ejecutarás auditorías reales, automatizarás umbrales de calidad e integrarás retroalimentación de complejidad directamente en tu editor y pull requests. Ya sea que trabajes de forma individual o en equipo, te llevarás herramientas y estrategias para escribir código Python más limpio y mantenible desde hoy."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Code Quality", "Code Readability", "Software Maintainability"],
    "level": "all",
    "video_url": ""
  },
  {
    "id": 46,
    "title": {
      "en": "Applied Test Driven Development with Python",
      "es": "Desarrollo basado en pruebas aplicado con Python"
    },
    "speakers": ["sebastian-rodriguez-colina"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "es": "En este taller exploraremos cómo aplicar Test Driven Development (TDD) en aplicaciones que se desarrollen en Python. Nos enfocaremos en retos extraídos de proyectos reales, para que puedas tener una buena idea de cómo aplicarlo en tu trabajo. Veremos además cómo el uso de TDD mejora significativamente la productividad del equipo, reduciendo la carga operativa de los ingenieros. El taller está dirigido a desarrolladores que buscan mejorar su calidad de código, acelerar su entrega de valor y reducir deuda técnica desde el primer commit.",
      "en": "In this workshop, we will explore how to apply Test Driven Development (TDD) in Python-based applications. We’ll focus on challenges extracted from real-world projects to give you a clear idea of how to use TDD effectively at work. We will also examine how TDD significantly improves team productivity by reducing engineers’ operational burden. This workshop is aimed at developers looking to improve code quality, speed up value delivery, and reduce technical debt from the very first commit."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Core Python", "Devops", "Web", "Software Engineering"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 47,
    "title": {
      "en": "Build AI Agents with LangGraph and MCP Web Scraping",
      "es": "Construir agentes de IA con LangGraph y MCP Web Scraping"
    },
    "speakers": ["santiago-suarez-sampayo"],
    "spoken_language": "english",
    "submission": "workshop",
    "description": {
      "en": "In this hands-on workshop, attendees will build a Python-based autonomous AI agent that scrapes multiple online shoe stores, extracts prices for a chosen product, and selects the best deal, fully automated using LangGraph and Model Context Protocol (MCP).\n\nAttendees will learn to design robust, stateful, and multi-agent architectures using LangGraph, leveraging Python’s flexibility to fully customize agent workflows based on specific use cases. They will also explore how MCP simplifies the integration of tools for AI agents, such as web scraping, enabling seamless access to real-time data and allowing agents to make more informed, autonomous decisions in real-world scenarios.\n\nBy the end, attendees will leave with practical experience, ready-to-use code, and a strong foundation to start building their own AI agents capable of automating complex tasks and solving real-world problems.",
      "es": "En este taller práctico, los asistentes construirán un agente autónomo de inteligencia artificial basado en Python que recopila información de múltiples tiendas de zapatos en línea, extrae precios de un producto elegido y selecciona automáticamente la mejor oferta, utilizando LangGraph y el Model Context Protocol (MCP).\n\nLos participantes aprenderán a diseñar arquitecturas robustas, con estado y multiagente usando LangGraph, aprovechando la flexibilidad de Python para personalizar completamente los flujos de trabajo del agente según distintos casos de uso. También explorarán cómo MCP simplifica la integración de herramientas para agentes de IA, como el web scraping, lo que permite un acceso fluido a datos en tiempo real y decisiones más informadas y autónomas en escenarios reales.\n\nAl finalizar, los asistentes se llevarán experiencia práctica, código reutilizable y una base sólida para comenzar a construir sus propios agentes de IA capaces de automatizar tareas complejas y resolver problemas del mundo real."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Artificial intelligence", "AI Agents", "MCP"],
    "level": "advanced",
    "video_url": ""
  },
  {
    "id": 48,
    "title": {
      "en": "The magic behind ‘You might also be interested’: recommender systems in action",
      "es": "La magia detrás de 'Quizás también te interese': sistemas de recomendación en acción"
    },
    "speakers": ["nestor-ivan-arbelaez-lopez", "mateo-cano-solis"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "es": "En este workshop exploraremos el rol clave que cumplen los sistemas de recomendación en la personalización y el crecimiento de plataformas de ecommerce. Estas soluciones, basadas en Machine Learning e Inteligencia Artificial, permiten optimizar la experiencia del usuario y aumentar las ventas.\n\nComenzaremos con una introducción teórica sobre qué son y cómo funcionan, seguido de un análisis de las principales métricas utilizadas para evaluar su desempeño. En una parte práctica, los participantes implementarán sistemas de recomendación colaborativos y basados en contenido, entendiendo sus diferencias y aplicaciones reales.\n\nTambién presentaremos cómo en Mercado Libre llevamos estos modelos a producción mediante Fury, nuestra plataforma interna para escalar, desplegar y monitorear modelos de manera eficiente.\n\nComo cierre, revisaremos el estado del arte en sistemas de recomendación, incluyendo arquitecturas avanzadas como el Two-Tower Model, sistemas generativos y otras soluciones de ML que están transformando la personalización a gran escala.\n\nEste workshop combina teoría y práctica para brindar herramientas clave en el desarrollo, evaluación y despliegue de sistemas de recomendación efectivos.",
      "en": "In this workshop, we’ll explore the key role recommendation systems play in the personalization and growth of ecommerce platforms. These solutions, powered by Machine Learning and Artificial Intelligence, enhance user experience and drive sales.\n\nWe’ll begin with a theoretical overview of what recommendation systems are and how they work, followed by an analysis of the main metrics used to evaluate their performance. In the hands-on portion, participants will implement both collaborative and content-based recommendation systems, gaining a clear understanding of their differences and real-world use cases.\n\nWe’ll also share how these models are deployed at scale in production at Mercado Libre using Fury, our internal platform for efficiently scaling, deploying, and monitoring models.\n\nTo close, we’ll look at the state of the art in recommendation systems, including advanced architectures like the Two-Tower Model, generative systems, and other ML approaches that are reshaping large-scale personalization.\n\nThis workshop blends theory and practice to provide key tools for building, evaluating, and deploying effective recommendation systems."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Artificial intelligence", "Data Science", "Machine Learning"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 49,
    "title": {
      "en": "Optimal Decisions: Mathematical Models for Real Problems",
      "es": "Decisiones Óptimas: Modelos matemáticos para problemas reales"
    },
    "speakers": ["manuel-casado", "daniel-galvis"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "es": "Este taller es una introducción práctica al mundo de la optimización matemática aplicada con Python. Nuestro objetivo es ayudar a los asistentes a identificar cuándo realmente vale la pena optimizar un problema y cómo estructurarlo correctamente para obtener soluciones eficientes y útiles. A través de buenas prácticas de modelado, aprenderán a traducir situaciones reales en modelos matemáticos claros.\n\nLa optimización tiene un impacto directo en múltiples industrias: la logística puede mejorar rutas de transporte y distribución; el sector energético puede balancear oferta y demanda de recursos; la manufactura puede planificar su producción con mayor eficiencia; y las finanzas pueden optimizar portafolios y minimizar riesgos. Incluso en tecnología y servicios, se pueden tomar decisiones más inteligentes al asignar recursos o gestionar tiempos de respuesta. Al entender cómo identificar estos problemas y modelarlos adecuadamente, los asistentes podrán aplicar este enfoque en sus propios contextos.\n\nDurante el taller, utilizaremos Pyomo como herramienta de modelado en Python y Gurobi como solucionador para ejecutar los modelos. Todo lo aprendido se pondrá en práctica con un ejercicio que permitirá a los participantes experimentar de primera mano el poder de la optimización y cómo puede aplicarse en situaciones reales. Este taller está pensado para personas con conocimientos básicos de Python que quieran explorar cómo la optimización puede transformar la forma en que se resuelven problemas complejos.",
      "en": "This workshop is a practical introduction to the world of applied mathematical optimization with Python. Our goal is to help participants identify when it’s truly worth optimizing a problem and how to properly structure it to obtain efficient and useful solutions. Through good modeling practices, attendees will learn how to translate real-world situations into clear mathematical models.\n\nOptimization has a direct impact across multiple industries: logistics can improve transportation and distribution routes; the energy sector can balance supply and demand; manufacturing can plan production more efficiently; and finance can optimize portfolios and minimize risks. Even in technology and services, smarter decisions can be made by allocating resources or managing response times. By understanding how to identify and model these problems effectively, attendees will be able to apply this approach in their own domains.\n\nDuring the workshop, we’ll use Pyomo as the modeling tool in Python and Gurobi as the solver to execute the models. Everything learned will be put into practice through a hands-on exercise that will allow participants to experience the power of optimization and how it applies to real-world situations. This workshop is designed for people with basic Python knowledge who want to explore how optimization can transform the way complex problems are solved."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Data Science", "Scientific Computing", "Optimization"],
    "level": "intermediate",
    "video_url": ""
  },
  {
    "id": 50,
    "title": {
      "en": "Unlocking the Power of Elasticsearch with Python and MCP to Create Intelligent Searches",
      "es": "DLiberando el Potencial de Elasticsearch con Python y MCP para Crear Búsquedas Inteligentes"
    },
    "speakers": ["roberto-bedoya-garcia", "santiago-poveda-garcia"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "es": "Descubre el poder de la comunicación natural con tus datos a través del Model Context Protocol (MCP). En este workshop intensivo, explorarás cómo revolucionar la interacción con motores de búsqueda mediante la implementación de un servidor MCP que conecta Elasticsearch/OpenSearch con Python, permitiendo consultas y análisis de datos usando lenguaje natural.\n\n¿Qué aprenderás?\n\n**Parte Teórica: Fundamentos del MCP**\nComenzaremos desmitificando el Model Context Protocol, entendiendo su arquitectura, propósito y el impacto transformador que tiene en la forma como interactuamos con sistemas de búsqueda. Analizaremos cómo MCP actúa como puente entre la intención humana y la ejecución técnica, eliminando las barreras tradicionales del acceso a datos.\n\n**Parte Práctica: Implementación Hands-On**\nLos participantes construirán su propio servidor MCP que integra Elasticsearch/OpenSearch con Python, implementando herramientas para búsqueda de documentos, análisis de índices y gestión de clusters. La experiencia culmina con la capacidad de conversar naturalmente con tus datos - preguntando en español y obteniendo respuestas inteligentes y contextualizadas.\n\n**Experiencia Transformadora**\nEste taller no es solo sobre código; es sobre democratizar el acceso a la información. Imagina poder preguntarle a tu base de datos \"¿Cuáles fueron las tendencias de ventas el mes pasado?\" o \"Muéstrame los documentos relacionados con inteligencia artificial\" y recibir respuestas precisas y útiles.\n\n**Para quién es este workshop**\nDesarrolladores, analistas de datos, arquitectos de software y profesionales TI que buscan estar a la vanguardia de las tecnologías emergentes y comprender cómo la inteligencia artificial está redefiniendo la interacción con datos.\n\n**Requisitos:** Conocimientos básicos de Python y familiaridad con conceptos de búsqueda de datos.\n\nÚnete a esta experiencia donde la teoría se encuentra con la práctica, y donde cada participante sale con herramientas reales para transformar la manera en que sus organizaciones acceden y utilizan la información.",
      "en": "Discover the power of natural communication with your data through the Model Context Protocol (MCP). In this intensive workshop, you'll explore how to revolutionize interaction with search engines by implementing an MCP server that connects Elasticsearch/OpenSearch with Python, enabling queries and data analysis using natural language.\n\n**What you'll learn**\n\n**Theoretical Part: MCP Fundamentals**\nWe’ll start by demystifying the Model Context Protocol—understanding its architecture, purpose, and the transformational impact it brings to how we interact with search systems. We’ll explore how MCP acts as a bridge between human intent and technical execution, removing traditional barriers to data access.\n\n**Practical Part: Hands-On Implementation**\nParticipants will build their own MCP server that integrates Elasticsearch/OpenSearch with Python, implementing tools for document search, index analysis, and cluster management. The experience culminates in the ability to naturally converse with your data—asking questions in Spanish and receiving smart, contextualized responses.\n\n**A Transformative Experience**\nThis workshop isn’t just about code; it’s about democratizing access to information. Imagine asking your database “What were last month’s sales trends?” or “Show me documents related to artificial intelligence” and getting accurate, actionable answers.\n\n**Who is this for**\nDevelopers, data analysts, software architects, and IT professionals seeking to stay ahead of emerging technologies and understand how AI is redefining data interaction.\n\n**Requirements:** Basic knowledge of Python and familiarity with search engine concepts.\n\nJoin this experience where theory meets practice—and every participant leaves with real tools to transform how their organization accesses and uses information."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Artificial intelligence", "Community", "MCP", "Elastic"],
    "level": "all",
    "video_url": ""
  },
  {
    "id": 51,
    "title": {
      "en": "Creating Intelligent Agents with MCP and Low-Code: Generative AI to Solve Real Problems",
      "es": "Creación de Agentes Inteligentes con MCP y Low-Code: IA Generativa para resolver problemas reales."
    },
    "speakers": ["stiven-arteaga-bedoya", "socrates-martinez-berrio"],
    "spoken_language": "spanish",
    "submission": "workshop",
    "description": {
      "es": "Vivimos un momento en el que la inteligencia artificial generativa y las herramientas de automatización low-code están transformando radicalmente la manera en que resolvemos problemas. Lo que antes requería un equipo completo, hoy puede ser diseñado por una sola persona que combine pensamiento estratégico, modelos de IA y flujos automatizados.\n\nEste taller será una guía práctica paso a paso para construir un agente inteligente funcional desde cero. Empezaremos con una breve introducción conceptual sobre modelos generativos y su aplicación en la resolución de tareas específicas. A continuación, mostraremos cómo diseñar un flujo conversacional eficaz mediante ingeniería de prompts y configuración de tareas dinámicas.\n\nLuego, conectaremos ese agente con servicios reales (como Google Drive, Office 365 o Postgres) usando herramientas low-code como n8n o Power Automate, sin necesidad de escribir integraciones complejas. Aprenderemos a crear disparadores, consumir APIs y encadenar respuestas generadas por IA con acciones automatizadas.\n\nDurante la sesión, cada participante podrá seguir la construcción de un caso real: desde la toma de requerimientos, el diseño modular del flujo, la construcción del agente, hasta su prueba y evaluación. Al final, entregaremos recursos prácticos: plantillas editables, buenas prácticas de seguridad, y ejemplos de implementación en sectores como educación, clasificación documental o soporte automatizado.\n\nEste taller no es una charla sobre el potencial de la IA: es una experiencia guiada para aprender haciendo, con herramientas accesibles, casos reales y acompañamiento técnico constante.",
      "en": "We’re living in a moment where generative AI and low-code automation tools are radically transforming how we solve problems. What once required an entire team can now be designed by a single person combining strategic thinking, AI models, and automated workflows.\n\nThis workshop is a hands-on, step-by-step guide to building a fully functional intelligent agent from scratch. We'll start with a brief conceptual introduction to generative models and how to apply them to solve specific tasks. Then, we'll demonstrate how to design an effective conversational flow using prompt engineering and dynamic task configuration.\n\nNext, we’ll connect the agent to real-world services (such as Google Drive, Office 365, or Postgres) using low-code tools like n8n or Power Automate, without the need for complex integration code. You’ll learn to create triggers, consume APIs, and chain AI-generated responses to automated actions.\n\nThroughout the session, participants will follow the development of a real use case—from gathering requirements, modular flow design, agent construction, to testing and evaluation. By the end, you’ll walk away with practical resources: editable templates, security best practices, and real-world examples in areas like education, document classification, or automated support.\n\nThis is not just a talk about AI’s potential—it’s a guided experience to learn by doing, using accessible tools, real examples, and hands-on technical support."
    },
    "summary": { "en": "", "es": "" },
    "tags": ["Artificial intelligence"],
    "level": "all",
    "video_url": ""
  }
]
